---
title: "Applied Bayesian Modelling"
subtitle: "Section 1"
author: "Dr Joshua J Bon"
bibliography: refs.bib
format:
  revealjs:
    html-math-method: katex
    theme: default
    df-print: paged
    incremental: false 
    css: style.css
    chalkboard: true
    logo: dauphine-logo-white.png
editor: source
---


## Getting started

Applied Bayesian modelling
 - Statistical inference, prediction
 - Bayesian models
 - Computation
 - Workflows


# Section 1: Bayesian statistics

## Example 1: Députés debut age

```{r dep-debut}
#| echo: true
#| code-fold: true

library(readr)
library(ggplot2)

deputes <- read_csv(file = "data/deputes2019.csv") 
# change location to match folder of 'deputes2019.csv' on your computer

ggplot(deputes) + 
  geom_histogram(aes(x = (premier_elu - date_naissance)/356.25)) +
  scale_x_continuous("Debut age") +
  scale_y_continuous("Number of politicians") +
  theme_bw()

```

## Statistical inference and prediction

:::{.callout-note}
## Lay question 
What age do politicians make their debut in parliament?
:::

:::{.callout-tip}
## Statistical questions

1. What is the _average_ age at debut?
2. What is the _variance_ in age at debut?
3. What _statistical model_ best explains the distribution of question numbers? 
:::

- In France, according to the 2019 data!

## Bayes rule

Two events $A$ and $B$

$$\Pr(A\mid B) = \frac{\Pr(B \mid A) \Pr(A)}{\Pr(B)}$$

## Bayes theorem for inference

- Assume observed data: $X \sim P_\theta$ for some unknown $\theta$.
- Assume we have a prior belief for $\theta \sim \Pi$ 

:::{.callout-tip}
## Posterior distribution

$$\Pi(\mathrm{d}\theta \mid X=x) = \frac{p(x \mid \theta)\Pi(\mathrm{d}\theta)}{p(x)}$$

:::

## Bayes theorem for inference

:::{.callout-tip}
## Posterior density
$$\pi(\theta \mid x) = \frac{p(x \mid \theta)\pi(\theta)}{Z}$$
:::
-  $p(x \mid \theta)$ is density^[Probability density (mass) function (PDF/PMF).] of $P_\theta$ at $x$ for a given $\theta$.
- $\pi(\theta)$ is prior density of $\theta$.
- $Z$ is normalising constant (or model evidence)

## Bayes theorem for inference

:::{.callout-tip}
## Posterior density
$$\pi(\theta \mid x) = \frac{p(x \mid \theta)\pi(\theta)}{Z}$$
:::

- Marginal likelihood
  - $Z = p(x) = \int_\Theta p(x \mid \theta) \Pi(\mathrm{d}\theta)$
- Likelihood
  - $L(\theta) = p(x \mid \theta)$ for fixed $x$.
  
## Posterior quantities

- Moments
- Median, mode
- Credible intervals (quantiles)
- Posterior predictive distribution

## Demonstration: Posterior (Ex 1) 

- Age first elected $X_i$ for $i\in\{1,2,\ldots,565\}$

Assume model and prior

- $X_i \mid \beta \overset{\text{iid}}{\sim} \text{Gamma}(\alpha,\beta)$
- $\alpha = 200$ fixed
- $\beta \sim \text{Gamma}(\alpha_0,\beta_0)$


## Choice of prior

$$\Pi(\mathrm{d}\theta)$$

- Incorporate prior knowledge
  - Support of parameter
  - Moments
  - Quantiles
  - Prior predictive distribution
- Hyper priors
- Computationally tractable

## Conjugate priors

A prior is a _conjugate prior_ if given a likelihood, the prior and posterior are in the same family of distributions.

For known distributions, conjugate priors are _tractable_.

- Gamma prior on rate is conjugate prior for Gamma likelihood (fixed scale)
- Normal prior on mean is conjugate prior for Normal likelihood (fixed variance)
- Inverse Gamma prior on variance is conjugate prior for Normal likelihood (fixed mean)


## Flat/uniform priors

$$\pi(\theta) \propto 1$$

$\Pi$ may be an **improper prior**, $\int_{\Theta}\pi(\theta) = \infty$.

Flat priors may still contain information^[See "The prior can often only be understood in the context of the likelihood", [@gelman2017prior] ]

## Non-informative and weakly-informative priors

- Diffuse priors, e.g. $\theta \sim \text{N}(0, \sigma^2)$ with large $\sigma^2$
  - Note: $\sigma^2 \rightarrow \infty$ is a flat improper prior
- Priors with invariance, symmetry, uncertainty principles
  - Jefferys priors^[[@robert2009harold]]
  - Reference priors^[[@berger2009formal]]
  - Objective priors^[[@ghosh2011objective]]


## Jefferys priors

Recall Fisher information matrix for regular models

$$\mathcal{I}(\theta) = -\mathbb{E}\left[ \frac{\partial^2 \log L}{\partial \theta^2} \right]$$


$$\pi(\theta) \propto |\mathcal{I}(\theta)|^{1/2}$$

Property: for continuous parameters, invariance under reparameterisation

May be improper

## Posterior Computation

What if I don't use a conjugate prior? Intractable normalising constant. Why?

$$Z = \int_\Theta p(x \mid \theta) \Pi(\mathrm{d}\theta)$$

Use computational methods:

- MCMC: Markov chain Monte Carlo
- IS: Importance sampling
- SMC: Sequential Monte Carlo

## Choice of model

$$P_\theta$$
How are the data distributed? Is this a good approximation?

- Incorporate data knowledge
  - Support of data
  - Properties of the data
    - Groups
    - Dynamics
  - Prior predictive distribution
  
## Model checking I: Bayes Factors

$$Z_i = \int_\Theta p_i(x \mid \theta) \Pi_i(\mathrm{d}\theta) = \mathbb{E}_{\theta\sim\Pi_i}\left[ p_i(x\mid\theta)\right]$$

Model evidence = Expected likelihood of the observed data w.r.t. the prior for $M_i$

Bayes factor $\text{BF}(M_1,M_2) = \frac{Z_1}{Z_2}$

## Model checking I: Bayes Factors

Interpretation of Bayes factors

$$
\text{BF}(M_1,M_2) = \frac{\Pr(M_1\mid x)}{\Pr(M_2\mid x)} = \frac{Z_1}{Z_2}
$$

## Example 2: Députés Questions

```{r dep-questions}
#| echo: true
#| code-fold: true

library(readr)
library(ggplot2)

deputes <- read_csv(file = "data/deputes2019.csv") 
# change location to match folder of 'deputes2019.csv' on your computer

ggplot(deputes) + 
  geom_histogram(aes(x = questions_orales)) +
  facet_grid(rows = "sexe") + 
  theme_bw()

```

## Example 2: Statistical inference

:::{.callout-note}
## Lay question 
Do the number of questions politicians ask differ by gender?
:::

:::{.callout-tip}
## Statistical questions

1. Does the _average_ number of questions differ by gender?
2. Does the _variance_ in the number of questions differ by gender?
3. What _statistical model_ best explains the distribution of question numbers? 
:::


## Monte Carlo integration

$$\mathbb{E}_{\theta\sim P}\left[f(\theta)\right] \approx \frac{1}{N}\sum_{i=1}^{N}f(\theta_i), \quad \text{where}~\theta_i \overset{\text{iid}}{\sim} P$$

In the sense that $\frac{1}{N}\sum_{i=1}^{N}f(\theta_i) \overset{\text{P}}{\rightarrow} \mathbb{E}_{\theta\sim P}\left[f(\theta)\right] $ as $n\rightarrow \infty$ if $\mathbb{E}_{\theta\sim P}\left[f(\theta)\right]  < \infty$.

## Importance sampling

$$\mathbb{E}_{\theta\sim P}\left[f(\theta)\right] = \mathbb{E}_{\theta\sim Q}\left[f(\theta)\frac{p(\theta)}{q(\theta)}\right] \approx \frac{1}{N}\sum_{i=1}^{N}f(\theta_i)\frac{p(\theta_i)}{q(\theta_i)}, \quad \text{where}~\theta_i \overset{\text{iid}}{\sim} Q$$

In the sense that $\frac{1}{N}\sum_{i=1}^{N}f(\theta_i)\frac{p(\theta_i)}{q(\theta_i)} \overset{\text{P}}{\rightarrow} \mathbb{E}_{\theta\sim P}\left[f(\theta)\right] $ as $n\rightarrow \infty$ if $\mathbb{E}_{\theta\sim Q}\left[f(\theta)\frac{p(\theta)}{q(\theta)}\right]  < \infty$.

## Recap: Ingredients of Bayesian models

1. Prior
2. Model
3. Computation

## References

::: {#refs}
:::

# Appendices