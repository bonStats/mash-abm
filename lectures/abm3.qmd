---
title: "Gibbs samplers and variable selection"
subtitle: "Applied Bayesian Modelling: Lecture 3"
author: "Dr Joshua J Bon"
bibliography: ../assets/refs.bib
format:
  revealjs:
    html-math-method: katex
    theme: default
    df-print: paged
    incremental: false 
    css: style.css
    chalkboard: true
    logo: ../assets/dauphine-logo-white.png
    footer: "[:arrow_up: ABM](../index.qmd)"
    cache: true
editor: source
from: markdown+emoji
---

```{r setup}

josh_data_dir <- function(fl){
  paste0("/Users/jbon/github/mash-abm/data/", fl)
}

```

## Applied Bayesian modelling

-   Statistical inference :mag:
-   Statistical prediction :mag:
-   Bayesian models
-   Computation :mag:
-   Workflows

# Statistical inference

## Example 3: Explaining death rates


```{r death-plot}
#| echo: true
#| code-fold: true

library(readr)
library(dplyr)
library(ggplot2)
library(GGally)

deathrate <- read_csv(file = josh_data_dir("deathrate2.csv"), col_types = cols(col_skip(), col_guess()))
# skip the first column
# Download: https://github.com/bonStats/mash-abm/tree/main/data
# change location to match folder of 'deathrate2.csvs' on your computer
# e.g. "~/Downloads/deathrate2.csv" or "C:/Users/[User Name]/Downloads/deathrate2.csv"

#deathrate 

deathrate %>% 
  select(1:8) %>%
  ggpairs(progress = F)

```

## Example 3: Explaining death rates


```{r death-plot-2}
#| echo: true
#| code-fold: true

deathrate %>% 
  select(c(1,9:16)) %>%
  ggpairs(progress = F)

```

## Example 3: Explaining death rates


```{r death-plot-3}
#| echo: true
#| code-fold: true

deathrate %>% 
  select(c(1,starts_with("noise"))) %>%
  ggpairs(progress = F)

```

## Linear regression

$$y = X \beta + \epsilon$$
$$y \in \mathbb{R}^n, \quad \beta \in \mathbb{R}^{p+1}, \quad \epsilon_i \overset{\text{iid}}{\sim} \mathcal{N}(0, \sigma^2)$$

$$X = \begin{bmatrix}
  \mid  & \mid & \mid & &\mid \\
  1  & x_1 & x_2 & \cdots & x_p \\
  \mid  & \mid & \mid & &\mid
  \end{bmatrix}$$

Typically $n \gg p$, and no spurious variables $y$... 

what to do if this is not the case?

## Variable selection

- Forward/backward selection
  - Hypothesis testing (Frequentist)
  - Bayes factors
- Bayes model averaging
- Marginal posterior inclusion probabilities

# Posterior predictive

## Posterior predictive

If we have prior data model $P(\cdot\mid\theta)$ and posterior $\Pi(\cdot\mid y)$

The posterior predictive distribution is $y^\ast \sim P^\ast(\cdot\mid y)$ where

$$P^\ast (\mathrm{d}y^\ast \mid y) = \int_{\Theta}P(\mathrm{d}y^\ast\mid\theta) \Pi( \mathrm{d}\theta \mid y)$$

## Posterior predictive samples

If we have samples $\theta_i \sim \Pi( \cdot \mid y)$ then

$y^\ast_i \sim P^\ast(\cdot\mid \theta_i)$ will have posterior predictive (marginal) distribution

Note: Marginal distribution of samples


# Gibbs samplers

## Gibb samplers

A Markov chain Monte Carlo technique, uses conditional distributions to build an invariant transition kernel

Why?

Conditional distributions may have known analytical forms... exact sampling (from conditionals)

## Posterior conditionals

Let $\dim (\theta) = d$ with posterior density $\pi(\theta \mid y)$

Separate elements of $\theta$ using indices partition $S_1 \cup S_2 \cup \cdots \cup S_k = \{1,2,\ldots,d\}$, $2 \leq k \leq d$ 

Conditional distributions are

$$\begin{matrix}\pi(\theta^{S_1}\mid y, \theta^{\bar{S_1}}) \\ \pi(\theta^{S_2}\mid y, \theta^{\bar{S_2}}) \\ \vdots \\ \pi(\theta^{S_k}\mid y, \theta^{\bar{S_k}})   \end{matrix}$$

## Posterior full conditionals

If $S_i=\{i\}$ the conditional distributions are $\pi(\theta^{\{i\}}\mid y, \theta^{\overline{\{i\}}}) = \pi(\theta^{i}\mid y, \theta^{-i})$ (one possible ordering)

For example with $k=3$,

$$\begin{matrix}\pi(\theta^{\{1\}}\mid y, \theta^{\{2,3\}}) \\ \pi(\theta^{\{2\}}\mid y, \theta^{\{1,3\}}) \\ \vdots \\ \pi(\theta^{\{3\}}\mid y, \theta^{\{1,2\}})   \end{matrix}$$

## Gibb sampling algorithm

Initialise $\theta_0$. At time $t$:


1. Set $\vartheta = \theta_{t-1}$ 
2. For $i \in \{1,2\ldots,k\}$
  - Sample conditional posterior $\theta_t^{S_i} \sim \Pi(\cdot\mid y, \vartheta^{\bar{S}_i})$
  - Update $\vartheta^{S_i} = \theta_t^{S_i}$
  
Run until convergence (check diagnostics)


## Gibb sampling algorithm

Code example


## References

::: {#refs}
:::

# Appendices




