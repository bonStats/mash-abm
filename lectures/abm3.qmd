---
title: "Gibbs samplers and variable selection"
subtitle: "Applied Bayesian Modelling: Section 3"
author: "Dr Joshua J Bon"
bibliography: ../assets/refs.bib
format:
  revealjs:
    html-math-method: katex
    theme: default
    df-print: paged
    incremental: false 
    css: style.css
    chalkboard: true
    logo: ../assets/dauphine-logo-white.png
    footer: "[:arrow_up: ABM](../index.qmd)"
    cache: true
editor: source
from: markdown+emoji
---

{{< include ../snippets/_data-directory.qmd >}}

## Applied Bayesian modelling

-   Statistical inference :mag:
-   Statistical prediction :mag:
-   Bayesian models
-   Computation :mag:
-   Workflows

# Statistical inference

## Example 3: Explaining death rates


```{r death-plot}
#| echo: true
#| code-fold: true

library(readr)
library(dplyr)
library(ggplot2)
library(GGally)

deathrate <- read_csv(file = josh_data_dir("deathrate2.csv"), col_types = cols(col_skip(), col_guess()))
# skip the first column
# Download: https://github.com/bonStats/mash-abm/tree/main/data
# change location to match folder of 'deathrate2.csvs' on your computer
# e.g. "~/Downloads/deathrate2.csv" or "C:/Users/[User Name]/Downloads/deathrate2.csv"

#deathrate 

deathrate %>% 
  select(1:8) %>%
  ggpairs(progress = F)

```

## Example 3: Explaining death rates


```{r death-plot-2}
#| echo: true
#| code-fold: true

deathrate %>% 
  select(c(1,9:16)) %>%
  ggpairs(progress = F)

```

## Example 3: Explaining death rates


```{r death-plot-3}
#| echo: true
#| code-fold: true

deathrate %>% 
  select(c(1,starts_with("noise"))) %>%
  ggpairs(progress = F)

```

## Linear regression

$$y = X \beta + \epsilon$$
$$y \in \mathbb{R}^n, \quad \beta \in \mathbb{R}^{p+1}, \quad \epsilon_i \overset{\text{iid}}{\sim} \mathcal{N}(0, \sigma^2)$$

$$X = \begin{bmatrix}
  \mid  & \mid & \mid & &\mid \\
  1  & x_1 & x_2 & \cdots & x_p \\
  \mid  & \mid & \mid & &\mid
  \end{bmatrix}$$

Typically $n \gg p$, and no spurious variables $y$... 

what to do if this is not the case?

## Variable selection

- Forward/backward selection
  - Hypothesis testing (Frequentist)
  - Bayes factors
- Bayes model averaging
- Marginal posterior inclusion probabilities

# Posterior predictive

## Posterior predictive

If we have data model $P(\cdot\mid\theta)$ and posterior $\Pi(\cdot\mid y)$,

then the posterior predictive distribution is

$$P^\ast (\cdot \mid y) = \int_{\Theta}P(\cdot\mid\theta) \Pi( \mathrm{d}\theta \mid y)$$

## Posterior predictive samples

1. Sample $\theta_i \sim \Pi( \cdot \mid y)$

2. Sample $y^\ast_i \sim P(\cdot\mid \theta_i)$ 

Marginal distribution $y^\ast_i \mid y$ is posterior predictive, 

$$y^\ast_i \sim P^\ast (\cdot\mid y)$$


# Gibbs samplers

## Gibb samplers

- Divide-and-conquer
    - High-dim sampling problem $\Rightarrow$ several low-dim problems
- Rely on sampling conditional posterior distributions
    - Build invariant transition kernels for easier problems
    - Use exact conditional or conditional MH step

## Gibb samplers

Why?

1. Conditional distributions may have known analytical forms
    - Can perform exact sampling from conditionals
    - **no tuning parameters!**
2. Conditional distributions lower dimensional
    - Metropolis-Hastings can be very efficient
    - MH easier to tune in low dimension

## Posterior conditionals

Let $\dim (\theta) = d$ with posterior density $\pi(\theta \mid y)$

Separate elements of $\theta$ using indices partition

$S_1 \cup S_2 \cup \cdots \cup S_k = \{1,2,\ldots,d\}$, $2 \leq k \leq d$ 

Conditional distributions are

$$\begin{matrix}\pi(\theta^{S_1}\mid y, \theta^{\bar{S_1}}) \\ \pi(\theta^{S_2}\mid y, \theta^{\bar{S_2}}) \\ \vdots \\ \pi(\theta^{S_k}\mid y, \theta^{\bar{S_k}})   \end{matrix}$$

## Posterior full conditionals

If $S_i=\{i\}$ the conditional distributions are 

$\pi(\theta^{\{i\}}\mid y, \theta^{\overline{\{i\}}}) = \pi(\theta^{i}\mid y, \theta^{-i})$

For example with $k=3$, one possible ordering is

$$\begin{matrix}\pi(\theta^{\{1\}}\mid y, \theta^{\{2,3\}}) \\ \pi(\theta^{\{2\}}\mid y, \theta^{\{1,3\}}) \\ \vdots \\ \pi(\theta^{\{3\}}\mid y, \theta^{\{1,2\}})   \end{matrix}$$

## Gibb sampling algorithm

Initialise $\theta_0$. At time $t$:


1. Set $\vartheta = \theta_{t-1}$ 
2. For $i \in \{1,2\ldots,k\}$
  - Sample conditional posterior $\theta_t^{S_i} \sim \Pi(\cdot\mid y, \vartheta^{\bar{S}_i})$
  - Update $\vartheta^{S_i} = \theta_t^{S_i}$
  
Run until convergence (check diagnostics)


## Gibb sampling algorithm (Ex)

Consider posterior distribution, conditional on summary statistics $s$ and $t$ (and number of data points $n$).

$$\pi(\alpha,\beta\mid s,t) \propto {n\choose \alpha} \beta^{\alpha+s-1}(1-\beta)^{n-\alpha+t-1}$$

for $\alpha \in \{0,1,\ldots,n\}$ and $\beta \in [0,1]$

@casella1992explaining

## Gibb sampling algorithm (Ex)

The conditional distributions are

$$\begin{aligned}
\alpha\mid\beta,s,t &\sim \text{Bin}(n, \beta) \\
\beta\mid\alpha,s,t &\sim \text{Beta}(\alpha+s, n-\alpha+t)
\end{aligned}$$

## Gibb sampling algorithm (Ex) {.scrollable}

```{r gibbs-ex}
#| echo: true
#| code-fold: true

library(dygraphs)

sample_alpha_conditional <- function(beta, n) {
  
  rbinom(n = 1, size = n, prob = beta)
  
}

sample_beta_conditional <- function(alpha, s, t, n) {
  
  rbeta(n = 1, shape1 = alpha+s, shape2 = n-alpha+t)

  }

gibbs <- function(N, beta0, data, burnin = 1){
  
  alphas <- rep(0, N+1)
  betas <- rep(beta0, N+1)
  
  for (t in 2:(N+1)){
    alphas[t] <- sample_alpha_conditional(betas[t-1], data$n)
    betas[t] <- sample_beta_conditional(alphas[t], data$s, data$t, data$n)
  }
  
  return(tibble(alpha = alphas[(burnin+1):N], 
                beta = betas[(burnin+1):N], 
                iter = (burnin+1):N)
         )
}

N <- 5000
data <- list(n = 5, s = 2, t = 1)

samples <- gibbs(N, 0.5, data, burnin = 100)

# ggplot(samples) +
#   geom_line(aes(x=iter, y = beta)) +
#   theme_bw()

dygraph(ts(samples$alpha,start = 1), ylab = "Alpha") %>% dyRangeSelector()

```

```{r gibbs-ex-2}
#| echo: false
#| code-fold: false

dygraph(ts(samples$beta,start = 1), ylab = "Beta") %>% dyRangeSelector()

```

# Model averaging

## Zellner’s G-prior: Probability model

#### Prior

$$\begin{aligned}
\beta \mid \sigma^2, X &\sim \mathcal N\left(0, g\sigma^2(X^\top X)^{-1}\right)\\
\pi(\sigma^2)&\propto \sigma^{-2} \end{aligned}$$

#### Data model

$$y \mid \beta, \sigma^2,X \sim \mathcal{N}(X \beta, \sigma^2)$$
with fixed hyper-parameter $g$ [@zellner1986gprior]

## Zellner’s G-prior: Posterior

Zellner’s G-prior leads to a posterior

$$\begin{aligned}
\beta \mid \sigma^2,y,X&\sim \mathcal{N}\left(\frac{g}{g+1}\hat\beta,
\frac{\sigma^2g}{g+1}(X^\top X)^{-1}\right) \\
\sigma^2\mid y,X &\sim \mathcal{IG}\left(\frac{n}{2},\frac{s^2}{2}+\frac{1}{2(g+1)}
\hat\beta^\top X^\top X\hat\beta\right)   \end{aligned}$$

## Zellner’s G-prior: Model evidence

$$\begin{aligned} m(y\mid X) = & (g+1)^{-(p+1)/2}\pi^{-n/2}\Gamma(n/2) \\ & \times\left[y^\top y-\frac{g}{g+1}y^\top X(X^\top X)^{-1}X^\top y\right]^{-n/2}\end{aligned}$$

## Zellner’s G-prior: Exact samples

For^[Sample in parallel if desired] $i\in\{1,2,\ldots,N\}$

1. Sample $\sigma^2_i \mid y,X$ from IG
2. Sample $\beta_i \mid \sigma^2_i,y,X$ from MVN

Results in exact posterior draws $\theta_i = (\beta_i,\sigma^2_i)$

## Model averaging with Zellner’s G-prior

Model choice involves selecting best set of covariates

Bayesian model averaging finds best subset of models with uncertainty

- $\gamma_i \in \{0,1\}$ for $i \in \{0,1,\ldots,p\}$
- $\gamma=(\gamma_0,\gamma_1, \gamma_2, \ldots, \gamma_p)$
- $X_\gamma$ be the submatrix of $X$ which only includes covariate $x_i$ as a column when $\gamma_i=1$
- Always keep the intercept, $\gamma_0=1$ fixed

## Probability model with averaging (V1)

#### Prior

$$\begin{aligned}
\beta_\gamma \mid \sigma^2, \gamma, X &\sim \mathcal N\left(0, g\sigma^2(X_\gamma^\top X_\gamma)^{-1}\right),\\
\pi(\sigma^2\mid \gamma)&\propto \sigma^{-2} \\
\gamma_i & \overset{\text{iid}}{\sim} \text{Bern}(\rho), \quad i\in\{1,2,\ldots,p\}
\end{aligned}$$

#### Data model

$$y \mid \beta,\sigma^2,\gamma,X \sim \mathcal{N}(X_\gamma \beta_\gamma, \sigma^2)$$

## Probability model with averaging (V2)

#### Prior

$$\begin{aligned}
\beta_\gamma \mid \sigma^2, \gamma, X &\sim \mathcal N\left(0, g\sigma^2(X_\gamma^\top X_\gamma)^{-1}\right),\quad \beta_{1-\gamma} \vert \gamma = 0 \\
\pi(\sigma^2\mid \gamma)&\propto \sigma^{-2} \\
\gamma_i & \overset{\text{iid}}{\sim} \text{Bern}(\rho), \quad i\in\{1,2,\ldots,p\}
\end{aligned}$$

#### Data model

$$y \mid \beta,\sigma^2,X \sim \mathcal{N}(X \beta, \sigma^2)$$


## Conditional posterior density

We index each model using $\gamma$ vector. The model $\mathcal{M}_\gamma$ has posterior density

$$\pi(\beta,\sigma^2 \mid \gamma,y,X)$$

- If $\gamma_i = 0$ then $\beta_i=0$, only coordinates $\beta_\gamma$ are "active"
- $\gamma_0 = 1$ fixed to always include intercept

## Conditional posterior distribution

The model $\mathcal{M}_\gamma$ has conditional distribution

$$\begin{aligned}
\beta_{\gamma} \mid \sigma^2,\gamma,y,X&\sim \mathcal{N}\left(\frac{g}{g+1}\hat\beta_\gamma,
\frac{\sigma^2g}{g+1}(X_\gamma^\top X_\gamma)^{-1}\right) \\
\sigma^2\mid \gamma, y,X &\sim \mathcal{IG}\left(\frac{n}{2},\frac{s^2_\gamma}{2}+\frac{1}{2(g+1)}
\hat\beta_\gamma^\top X_\gamma^\top X_\gamma\hat\beta_\gamma\right)   \end{aligned}$$

with $\beta_{1-\gamma} =0$

## Model evidence

The model evidence for $\mathcal{M}_\gamma$ is 

$$\begin{aligned} m(y\mid \gamma,X) = & (g+1)^{-p_\gamma/2}\pi^{-n/2}\Gamma(n/2) \\ & \times\left[y^\top y-\frac{g}{g+1}y^\top X_\gamma(X_\gamma^\top X_\gamma)^{-1}X_\gamma^\top y\right]^{-n/2}\end{aligned}$$

where^[Recall $\gamma_0=1$ is fixed.] $p_\gamma = \sum_{i=0}^p\gamma_i$.



## Full posterior over models

The posterior density factorises as

$$\pi(\beta, \sigma^2, \gamma \mid X,y) = \pi(\beta, \sigma^2 \mid \gamma, X,y) \pi(\gamma \mid X,y) $$

#### Full posterior sampling strategy

- Sample over models $\mathcal{M}_\gamma$ by targetting $\gamma \mid X,y$
    - Gibbs sampler on conditionals: $\gamma^i \mid X,y,\gamma^{-i}$

- Use exact sampling for $\beta, \sigma^2 \mid \gamma, X,y$

## Marginal posterior for models

$$\begin{aligned}\pi(\gamma \mid X,y) &= \frac{\pi(\beta, \sigma^2, \gamma \mid X,y)}{\pi(\beta, \sigma^2 \mid \gamma, X,y)}\\
&~~\vdots \\
& \propto m(y\mid \gamma,X) \pi(\gamma) \end{aligned}$$


where $\pi(\gamma)$ is prior PMF for $\gamma$.

## References

::: {#refs}
:::

# Appendices




