---
title: "MCMC & Metropolis-Hastings"
subtitle: "Applied Bayesian Modelling: Lecture 2"
author: "Dr Joshua J Bon"
bibliography: ../assets/refs.bib
format:
  revealjs:
    html-math-method: katex
    theme: default
    df-print: paged
    incremental: false 
    css: style.css
    chalkboard: true
    logo: ../assets/dauphine-logo-white.png
    footer: "[:arrow_up: ABM](../index.qmd)"
    cache: true
editor: source
from: markdown+emoji
---

```{r setup}

josh_data_dir <- function(fl){
  paste0("/Users/jbon/github/mash-abm/data/", fl)
}

```

## Applied Bayesian modelling

-   Statistical inference :mag:
-   Statistical prediction
-   Bayesian models
-   Computation :mag:
-   Workflows

::: callout-note
## Links 

- [Golf](https://github.com/bonStats/mash-abm/tree/main/data){target="_blank"}
- [Index slides](../index.html)
:::

# Statistical inference

## Example 2: Golf data 

```{r golf-plot}
#| echo: true
#| code-fold: true

library(readr)
library(dplyr)
library(ggplot2)

golf <- read_csv(file = josh_data_dir("golf2.txt"))
# Download: https://github.com/bonStats/mash-abm/tree/main/data
# change location to match folder of 'golf2.txt' on your computer
# e.g. "~/Downloads/golf2.txt" or "C:/Users/[User Name]/Downloads/golf2.txt"

golf %>% group_by(distance) %>% summarise(prop_success = mean(success)) %>%
ggplot() + 
  geom_col(aes(x = distance, y = prop_success)) +
  scale_x_continuous("Distance to hole") +
  scale_y_continuous("Proportion successful") +
  theme_bw()

```

## Statistical inference

::: callout-note
## Lay question

What determines accuracy in golf putting?
:::

::: callout-tip
## Statistical questions

1.  How likely is the player to successfully sink the putt at distance $x$ metres?
2.  How variable is the players angle of putting compared to the correct angle?

:::

-   According to this putting data from one player

## Logistic regression

Model for $Y_i \in \{0,1\}$, for $i \in \{1,\ldots,n\}$:

$$\Pr(Y_i = 1) = \frac{\exp\{\beta_0 + \beta_1x_i\}}{1 + \exp\{\beta_0 + \beta_1x_i\}}$$

- Covariate $x_i$ (can be many) 
- Unknown coefficients $\beta_0,\beta_1$

## Logistic regression

```{r logistic-regression}
#| echo: true
#| code-fold: true

n <- 20

b0 <- 0.5
b1 <- 2
link <- function(x){
  exp(b0 + b1 * x) / ( 1 + exp(b0 + b1 * x) )
}

simdata <- tibble(x = rnorm(n))

pr_y_1 <- link(simdata$x)

simdata <- simdata %>% mutate(y = rbinom(n, size = 1, prob = pr_y_1))

ggplot(simdata) + 
  geom_point(aes(x=x,y=y, colour = "Observed data")) + 
  stat_function(aes(colour = "Pr(Y=1)"), fun = link) + 
  scale_color_discrete("") + 
  theme_bw()

```


# Beyond MC and IS

## Recap: Monte Carlo approximation

$$\mathbb{E}_{\theta\sim P}\left[f(\theta)\right] \approx \frac{1}{N}\sum_{i=1}^{N}f(\theta_i), \quad \text{where}~\theta_i \overset{\text{iid}}{\sim} P$$

What if we can't sample from $P$?

## Self-normalised importance sampling

Extending IS when $\pi(\cdot\mid y)$ can't be evaluated due to $Z$.

$$\mathbb{E}_{\theta\sim \Pi(\cdot\mid y)}\left[f(\theta)\right] = \frac{\mathbb{E}_{\theta\sim Q}\left[f(\theta)w(\theta)\right]}{\mathbb{E}_{\theta\sim Q}\left[w(\theta)\right]}$$

where $w(\theta) = \frac{L(\theta\mid y)\pi(\theta)}{q(\theta)}$.

## Self-normalised importance sampling

$$\mathbb{E}_{\theta\sim \Pi(\cdot\mid y)}\left[f(\theta)\right] \approx \frac{1}{N}\sum_{i=1}^{N}f(\theta_i)\bar{w}_i$$


- $\theta_i \overset{\text{iid}}{\sim} Q$
- $\bar{w}_i = \frac{w(\theta_i)}{\sum_{j=1}^N w(\theta_j)}$
- $w(\theta) = \frac{L(\theta\mid y)\pi(\theta)}{q(\theta)} \propto \frac{\Pi(\theta\mid y)}{q(\theta)}$

## Markov chain Monte Carlo

Extending vanilla MC when $\theta \sim \Pi(\cdot\mid y)$ can't be sampled.

Construct Markov chain with limiting distribution $\Pi(\cdot\mid y)$

$$\theta_t \sim K(\cdot\mid \theta_{t-1}),\quad t\in\{1,2,\ldots,\}$$

for some $\theta_0$. 

$$\theta_n \sim K^n(\cdot\mid\theta_0)$$

for $\theta_n \overset{\text{d}}{\longrightarrow} \Pi(\cdot\mid y )$ as $n\rightarrow \infty$

## Markov chain Monte Carlo

We can estimate quantities with the MCMC samples

$$\mathbb{E}_{\theta\sim \Pi(\cdot\mid y)}\left[f(\theta)\right] \approx \sum_{t=1}^{N}f(\theta_t),\quad \theta_t \sim K(\cdot\mid \theta_{t-1})$$

What to use for $K$?

# Metropolis-Hastings

## The Metropolis-Hastings algorithm

Initialise $\theta_0$, at time $t$:

1. Sample proposal $\theta^\prime \sim Q(\cdot\mid\theta_{t-1})$
2. Calculate acceptance $\alpha = \min \left\{\frac{q(\theta_{t-1} \mid \theta^\prime)\pi(\theta^\prime\mid y)}{q(\theta^\prime \mid \theta_{t-1})\pi(\theta_{t-1}\mid y)},1 \right\}$
3. Draw $U \sim \text{Unif}(0,1)$
  - If $\alpha \geq U$ accept proposal, set $\theta_t = \theta^\prime$
  - Else $\alpha<U$ reject proposal, set $\theta_t = \theta_{t-1}$
  
Run until convergence (heuristic checks)

## MH: Acceptance rate

$$\alpha = \min \left\{\frac{q(\theta_{t-1} \mid \theta^\prime)\pi(\theta^\prime\mid y)}{q(\theta^\prime \mid \theta_{t-1})\pi(\theta_{t-1}\mid y)},1 \right\}$$
$$ = \min \left\{\frac{q(\theta_{t-1} \mid \theta^\prime)L(\theta^\prime\mid y)\pi(\theta^\prime)}{q(\theta^\prime \mid \theta_{t-1})L(\theta_{t-1}\mid y)\pi(\theta_{t-1})},1 \right\}$$
No normalising constant required!

Special case: Symmetric proposal

## Running a MH Algorithm

```{r mh-ex-trace}
#| echo: true
#| code-fold: true

library(dygraphs)

logprob_target <- function(theta){ # unnormalised if neccessary
  
  dnorm(theta, mean = 1, sd = 1, log = TRUE)
  
}

rproposal <- function(given_theta, tune_sd) {
  
  rnorm(1, mean = given_theta, sd = tune_sd)
  
}

logprob_proposal <- function(theta, given_theta, tune_sd){
  
  dnorm(theta, mean = given_theta, sd = tune_sd, log = TRUE)
  
}

mh_iteration <- function(given_theta, tune_sd){
  
  proposal <- rproposal(given_theta, tune_sd)
  log_uniform <- log(runif(1))
  
  log_alpha <- ( logprob_proposal(given_theta, proposal, tune_sd) + logprob_target(proposal) ) - 
                ( logprob_proposal(proposal, given_theta, tune_sd) + logprob_target(given_theta) )
  
  if (log_alpha >= log_uniform){
    next_theta <- proposal
  } else {
    next_theta <- given_theta
  }
  
  return(next_theta)
}

mh <- function(n, theta0, tune_sd, burnin = 0){
  
  thetas <- rep(theta0, n)
  
  for (t in 2:n){
    thetas[t] <- mh_iteration(thetas[t-1], tune_sd)
  }
  
  return(thetas[(burnin+1):n])
}

N <- 5000
simdata <- tibble(theta = mh(N, 0, 1), iter = 1:N)

# ggplot(simdata) + 
#   geom_line(aes(x=iter, y = theta)) + 
#   theme_bw()

dygraph(ts(simdata$theta,start = 1)) %>% dyRangeSelector()

```

# Diagnostics

## Diagnostics

Convergence is guaranteed in **theory** as $n\rightarrow \infty$,

**in practice** we need to monitor and check MCMC samples to get _close enough_!

## MCMC: Chain trace plot

```{r mh-ex-trace-comp}
#| echo: true
#| code-fold: true

# iterate over multiple settings
library(tidyr)
library(purrr)

tune_sds <- c(0.01,0.1,0.5,1.0)
ns <- c(200,1000,10000)
burnins <- c(0,100)

mh_vec <- Vectorize(mh, SIMPLIFY = F)

mhtest <- 
  expand_grid(n = ns, tune_sd = tune_sds, burnin = burnins) %>%
  mutate(theta = mh_vec(n = n, theta0 = 0, tune_sd = tune_sd, burnin = burnin)) %>%
  rowwise() %>% mutate(iter = list((burnin+1):n )) %>%
  unnest(c(theta, iter))

mhtest %>% filter(burnin == 100) %>% rename(sd = tune_sd) %>%
ggplot() + 
  geom_line(aes(x=iter, y = theta)) + 
  theme_bw() + 
  facet_grid(sd ~ n, scales = "free_x", labeller = label_both) +
  ggtitle("Burnin = 100")

```


## MCMC: Running mean trace plot

```{r mh-ex-mean}
#| echo: true
#| code-fold: true

mhtest %>% filter(burnin == 0) %>% rename(sd = tune_sd) %>%
  group_by(n, sd, burnin) %>% mutate(theta = cummean(theta)) %>%
ggplot() + 
  geom_line(aes(x=iter, y = theta)) + 
  geom_hline(yintercept = 1.0, linetype = 3) +
  theme_bw() + 
  facet_grid(sd ~ n, scales = "free_x", labeller = label_both) +
  ggtitle("Burnin = 0")


```

## MCMC: Running mean trace plot

```{r mh-ex-mean-2}
#| echo: true
#| code-fold: true

mhtest %>% filter(burnin == 100) %>% rename(sd = tune_sd) %>%
  group_by(n, sd, burnin) %>% mutate(theta = cummean(theta)) %>%
ggplot() + 
  geom_line(aes(x=iter, y = theta)) +
  geom_hline(yintercept = 1.0, linetype = 3) +
  theme_bw() + 
  facet_grid(sd ~ n, scales = "free_x", labeller = label_both) +
  ggtitle("Burnin = 100")


```


## MH: Optimal acceptance rate

$$\alpha = 0.234$$

- Justified as dim. $d \rightarrow \infty$ [@roberts2001optimal]
- Still often used in practice
- Specific to Metropolis-Hastings algorithm

## MH: Optimal acceptance rate

```{r mh-ex-trace-accept}
#| echo: true
#| code-fold: true

mhtest %>% filter(burnin == 100, n == 10000) %>% rename(sd = tune_sd) %>%
  mutate(accept = c(NA, abs(diff(theta, )) > 1e-08)) %>%
  filter(!is.na(accept)) %>% 
  group_by(sd) %>% 
  summarise(`Acceptance rate` = mean(accept)) %>%
ggplot() + 
  geom_point(aes(x=sd, y = `Acceptance rate`)) + 
  geom_hline(yintercept = 0.234, linetype = 3) +
  theme_bw()

```

# General MCMC convergence diagnostics

## Recap: Monte Carlo CLT

$$\sqrt{N} \left[ \frac{1}{N}\sum_{i=1}^{N}f(\theta_i) - \mathbb{E}_{\theta\sim P}\left[f(\theta)\right]\right] \overset{\text{d}}{\longrightarrow} N(0, \sigma^2_f)$$
if $\theta_i \overset{\text{iid}}{\sim} P$, and $\sigma^2_f = \text{Var}[f(\theta_i)] < \infty$.

- Vanilla Monte Carlo error has rate $\frac{\sigma_f}{\sqrt{N}} = \mathcal{O}(N^{-1/2})$

## MCMC CLT

$$\sqrt{N} \left[ \frac{1}{N}\sum_{t=1}^{N}f(\theta_t) - \mathbb{E}_{\theta\sim P}\left[f(\theta)\right]\right] \overset{\text{d}}{\longrightarrow} N\left(0, \sigma^2_f\sum_{t=-\infty}^\infty \rho_t\right)$$

- if $\theta_1 \sim P$, $\theta_t \sim K(\cdot\mid \theta_{t-1})$ for $t\in \{2,\ldots,N\}$
- $PK=P$
- $\sigma^2_f = \text{Var}[f(\theta_i)] < \infty$, $\rho_t = \frac{\text{cov}[f(\theta_1),f(\theta_{1+t})]}{\sigma^2_f}$


- MCMC error rate $\mathcal{O}(N^{-1/2})$ at stationarity


## MCMC CLT intuitions

Good MCMC schemes:

- Get to stationary distribution quickly (converge fast)
- Have small autocorrelation $\rho_t$

What quantitative measures exist to measure this?

## MCMC: Convergence $\hat{R}$ and friends

$\hat{R}$, potential scale reduction statistic [@gelman1992inference]

- Run multiple chains from different (random) starting points

- $\hat{R}$ measures the average variance of samples within each chain with respect to the variance of the pooled samples

- $\hat{R} \approx 1$ provides evidence of convergence

- $\hat{R} > 1$ indicates chains have not converged to a common distribution

## MCMC: Effective sample size

Vanilla Monte Carlo sample size  $=N$

MCMC effective sample size $N_\text{eff}=\frac{N}{\sum_{t=-\infty}^\infty\rho_t} = \frac{N}{1+2\sum_{t=1}^\infty\rho_t}$

- Estimate from MCMC samples (valid at stationarity)

## MH: Extensions

- Metropolis Adjusted Langevin Algorithm
- Adaptive Metropolis-Hastings

## References

::: {#refs}
:::

# Appendices
