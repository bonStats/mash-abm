---
title: "Bayesian workflow and Stan"
subtitle: "Applied Bayesian Modelling: Section 4"
author: "Dr Joshua J Bon"
bibliography: ../assets/refs.bib
format:
  revealjs:
    html-math-method: katex
    theme: default
    df-print: paged
    incremental: false 
    css: style.css
    chalkboard: true
    logo: ../assets/dauphine-logo-white.png
    footer: "[:arrow_up: ABM](../index.qmd)"
    cache: true
editor: source
from: markdown+emoji
---

{{< include ../snippets/_data-directory.qmd >}}

## Applied Bayesian modelling

-   Statistical inference :mag:
-   Statistical prediction 
-   Bayesian models
-   Computation :mag:
-   Workflows :mag:

# Statistical inference

## Example 4: Kidney Cancer Data

```{r kidney-plot}
#| echo: true
#| code-fold: true

library(readr)
library(dplyr)
library(usmap)
library(ggplot2)

kidney <- read_csv(file = josh_data_dir("kidneycancerclean.csv"), skip = 4, col_types = cols(col_skip(), col_guess()))
# change location to match folder of 'kidneycancerclean.csv' on your computer
# e.g. "~/Downloads/kidneycancerclean.csv" or 
# "C:/Users/[User Name]/Downloads/kidneycancerclean.csv"

kidney <- kidney %>% 
  mutate(total_deaths = dc + dc.2) %>% # 1980-1989
  mutate(population = (pop+pop.2)/2) %>% # average in 1980-1989
  mutate(theta_hat = total_deaths/(10*population)) %>% # estimate of annual death rate
  mutate(low_rate = theta_hat <= quantile(theta_hat, 0.1)) %>%
  mutate(high_rate = theta_hat >= quantile(theta_hat, 0.9))


plot_usmap("counties", data=kidney, values="high_rate") +
  scale_fill_discrete(h.start = 200, 
                      name = "High rate of kidney cancer deaths") 

```

## Example 4: Kidney Cancer Data

```{r kidney-plot2}
#| echo: true
#| code-fold: true

plot_usmap("counties", data=kidney, values="low_rate") +
  scale_fill_discrete(h.start = 200, 
                      name = "Low rate of kidney cancer deaths") 


```

## Example 4: Kidney Cancer Data

```{r kidney-plot3}
#| echo: true
#| code-fold: true

library(scales)

rate_code <- function(low_rate, high_rate){

  if(low_rate){
    return("low")
  } else if(high_rate) {
    return("high")
  } else {
    return("mid")  
  }
  
}

rate_codeV <- Vectorize(rate_code)

kidney <- kidney %>%
  mutate(rate = ordered(rate_codeV(low_rate, high_rate),c("low rate","mid rate","high rate")))

ggplot(data=kidney, aes(x=population)) + 
  geom_histogram(bins=30, fill="lightblue") + 
  facet_wrap(~ rate) +
  geom_vline(xintercept = 10^5, linetype = 3) + 
  scale_x_log10(label=trans_format("log10",math_format(10^.x))) +
  labs(x="Population size", 
       y="Number of counties")  

```

## What have we seen so far?

- Bayesian modelling
    - Building a model of the data
    - Choosing a prior distribution
    - Model selection

- Bayesian computation
    - Conjugate priors
    - MCMC: MH and Gibbs
    - MCMC diagnostics
    
## A simplified Bayesian workflow

<br>

```{mermaid}
%%| fig-align: center
flowchart TD
  B(Build Bayesian model) --> C(Compute posterior)
  C --> E(Evaluate posterior)
  E --> I[Conclusions]
  C -.-> B
  E -.-> B
  I --> Inf(Inference)
  I --> Pred(Prediction)
  I --> Dec(Decisions)
```

## A simplified Bayesian workflow

<br>

:::: {.columns}

::: {.column width="38%"}


#### Overall workflow

```{mermaid}
flowchart TD
  B(Build Bayesian model) --> C(Compute posterior)
  C --> E(Evaluate posterior)
  E --> I[Conclusions]
  C -.-> B
  E -.-> B
```

@gelman2020bayesian

:::

::: {.column width="5%"}

:::

::: {.column width="57%"}

#### Build Bayesian model
```{mermaid}
flowchart LR
  A(Propose likelihood) --> B( Propose prior)
  B --> C(Validate prior)
  C -.-> B
  C -.-> A
```

#### Compute posterior
```{mermaid}
flowchart LR
  A(Choose tool and settings) --> B(Fit model)
  B --> C(Validate approximation)
  C -.-> A
```

#### Evaluate posterior
```{mermaid}
flowchart LR
  A(Model checking) --> B(Model comparison)
  B --> C(Model choice)
```
:::

::::

## Build Bayesian model

<br> 

:::: {.columns}

::: {.column width="70%"}

#### Propose likelihood

How are the data generated?

- Support of distribution
- Dependency between data

:::

::: {.column width="30%"}

```{mermaid}
flowchart TD
  A(Propose likelihood) --> B( Propose prior)
  B --> C(Validate prior)
  C -.-> B
  C -.-> A
```

:::

::::

## Build Bayesian model

<br> 

:::: {.columns}

::: {.column width="70%"}

#### Propose prior

What prior information do we have on the model parameters (given the likelihood)?

- Global vs local parameters
- Constraints
- Expert elicitation

:::

::: {.column width="30%"}

```{mermaid}
flowchart TD
  A(Propose likelihood) --> B( Propose prior)
  B --> C(Validate prior)
  C -.-> B
  C -.-> A
```

:::

::::

## Build Bayesian model

<br> 

:::: {.columns}

::: {.column width="70%"}

#### Validate prior

Does the prior predictive distribution produce data that is reasonable?

- Prior predictive checking
- Domain knowledge

:::

::: {.column width="30%"}

```{mermaid}
flowchart TD
  A(Propose likelihood) --> B( Propose prior)
  B --> C(Validate prior)
  C -.-> B
  C -.-> A
```

:::

::::

## Compute posterior

<br> 

:::: {.columns}

::: {.column width="70%"}

#### Choose tool and settings

- Conjugate prior
- MC algorithm: MH, Gibbs, HMC^[Continuous parameters]
- MCMC settings: proposal distribution

:::

::: {.column width="30%"}

```{mermaid}
flowchart TD
  A(Choose tool and settings) --> B(Fit model)
  B --> C(Validate approximation)
  C -.-> A
```

:::

::::


## Compute posterior

<br> 

:::: {.columns}

::: {.column width="70%"}

#### Fit model

- MCMC software: R, Stan
- Initial number of iterations
- Save samples

:::

::: {.column width="30%"}

```{mermaid}
flowchart TD
  A(Choose tool and settings) --> B(Fit model)
  B --> C(Validate approximation)
  C -.-> A
```

:::

::::


## Compute posterior

<br> 

:::: {.columns}

::: {.column width="70%"}

#### Validate approximation

- Convergence diagnostics: $\hat{R}$, ESS
- Visualisations: Trace, autocorrelation plots

:::

::: {.column width="30%"}

```{mermaid}
flowchart TD
  A(Choose tool and settings) --> B(Fit model)
  B --> C(Validate approximation)
  C -.-> A
```

:::

::::


##

### The folk theorem of statistical computing

<br>

:::: {.columns}

::: {.column width="70%"}

When you have computational problems, often thereâ€™s a problem with your model 

-@yao2022stacking

:::

::: {.column width="30%"}

```{mermaid}
%%| fig-align: right
flowchart TD
  B(Build Bayesian model) --> C(Compute posterior)
  C --> E(Evaluate posterior)
  E --> I[Conclusions]
  C -.-> B
  E -.-> B
  
  linkStyle 3 stroke-width:4px, stroke:red
```

:::

::::

## Evaluate posterior

<br>

:::: {.columns}

::: {.column width="70%"}

#### Model checking

- Posterior predictive checks
- Prior predictive checks

[Stan guide](https://mc-stan.org/docs/stan-users-guide/posterior-predictive-checks.html)

[bayesplot guide](https://mc-stan.org/bayesplot/articles/graphical-ppcs.html)

:::

::: {.column width="30%"}

```{mermaid}
flowchart TD
  A(Model checking) --> B(Model comparison)
  B --> C(Model choice)
```

:::

::::

## Evaluate posterior

<br>

:::: {.columns}

::: {.column width="70%"}

#### Model comparison

- Model evidence
- Cross validation [@vehtari2017practical]

[Stan guide to {loo}](https://mc-stan.org/loo/articles/loo2-with-rstan.html)

:::

::: {.column width="30%"}

```{mermaid}
flowchart TD
  A(Model checking) --> B(Model comparison)
  B --> C(Model choice)
```

:::

::::

## Evaluate posterior

<br>

:::: {.columns}

::: {.column width="70%"}

#### Model choice

- Model selection
- Model averaging
- Posterior stacking [@yao2018using]


:::

::: {.column width="30%"}

```{mermaid}
flowchart TD
  A(Model checking) --> B(Model comparison)
  B --> C(Model choice)
```

:::

::::

# PPLs and Stan

## Probabilistic programming languages

- Syntax to encode joint probability model
- Creates joint probability object to be queried

```{mermaid}
%%| fig-align: center
flowchart LR
  A(PPL model specification) --> B(Joint probability object)
  B --> C(Inference algorithms)
```

PPLs are usually bundled with inference algorithm(s)

- MCMC (HMC!)
- VI
- SMC

## PPL examples

| Name                                 | PPL lang.            | Inference lang.      | Backend              |
|--------------------------------------|----------------------|----------------------|----------------------|
| BUGS family^[BUGS, WinBugs, OpenBUGS, JAGS]                         | "BUGS"               | various              |  various          |
| [Stan](https://mc-stan.org)          | Stan                 | R, Python, Julia,... | C++                  |
| [Turing.jl](https://turinglang.org)  | Julia                | Julia                | Julia
| [Pyro](http://pyro.ai)^[See also [numPyro](https://num.pyro.ai/) using JAX backend] | Python | Python | PyTorch |
| [PyMC](https://www.pymc.io/)         | Python               | Python               | PyTensor |
| [greta](https://greta-stats.org)     | R                    | R                    | TensorFlow |


## Bayesian models in Stan

:::: {.columns}

::: {.column width="40%"}

#### Probability model

Prior

$\alpha \sim \mathcal{N}(0,1)$

$\beta \sim \mathcal{N}(0,1)$
 
$\sigma \sim\mathcal{IG}(1,1)$

Likelihood 

$y_i \mid \alpha,\beta,\sigma, x \overset{\text{iid}}{\sim} \mathcal{N}(\alpha+\beta x_i,\sigma^2)$

:::

::: {.column width="60%"}

#### Stan code

```stan
data {
  int<lower=0> N;
  vector[N] x;
  vector[N] y;
}
parameters {
  real alpha;
  real beta;
  real<lower=0> sigma;
}
model {
  // prior
  alpha ~ normal(0, 1);
  beta ~ normal(0, 1);
  sigma ~ inv_gamma(1, 1);
  // likelihood
  y ~ normal(alpha + beta * x, sigma);
}
```

:::

::::

## Joint probaility object

```{mermaid}
%%| fig-align: center
flowchart LR
  A(Joint probability object)
  A --> C(Prior)
  C -.-> CA(log-prior density)
  A --> E(Data process)
  C --> F(Prior predictive)
  E --> F
  
  
  CA -.-> DA(unnormalised log-posterior density)
  E -.-> EA(log-likelihood)
  EA -.-> DA
```

$$\pi(\theta\mid y) \propto p(y, \theta) = p(y\mid\theta)\pi(\theta)$$

## Stan in R

```{r rstan-example}
#| echo: true
#| eval: false

library(rstan)

kidney_stan_data = list(N = nrow(kidney),
                  n = kidney$population,
                  y = kidney$total_deaths
                  )

fit_kidney = stan(file = "kidneycancer.stan", data = kidney_stan_data)

```

<br>

- `"kidneycancer.stan"` is a file with your stan model

## Installing rstan (as at 2024-01-12)


1. Update R to 4.4.x (and restart R)
2. Configure C++ Toolchain
   - Mac: Install [{macrtools}](https://github.com/coatless-mac/macrtools?tab=readme-ov-file#macrtools) and see [optimisation](https://github.com/stan-dev/rstan/wiki/Configuring-C---Toolchain-for-Mac)
   - Windows: Download [{rtools44}](https://cran.r-project.org/bin/windows/Rtools/rtools44/rtools.html) and see [guide](https://github.com/stan-dev/rstan/wiki/Configuring-C---Toolchain-for-Windows)
4. Install `rstan`, see [general guide](https://github.com/stan-dev/rstan/wiki/Rstan-Getting-Started) for more help
    - Mac: `install.packages("rstan")`
    - Windows: Install from source for R 4.4.x, `install.packages(c("StanHeaders", "rstan"), type = "source")`
    

## R packages built on Stan

- [`brms`](https://cran.r-project.org/package=brms)
- [`bayesforecast`](https://cran.r-project.org/package=bayesforecast)
- [`bayesdfa`](https://cran.r-project.org/package=bayesdfa)
- [`mvgam`](https://cran.r-project.org/package=mvgam)
- [`stan4bart`](https://cran.r-project.org/package=stan4bart)

## Crash-course in HMC

Hamiltonian Monte Carlo... see @betancourt2017conceptual

- Trajectories
- Leapfrog integrator
- Divergent transitions
- No-U-Turn criterion

# Bayes model evaluation with cross-validation

## Bayes model evaluation with elpd

elpd = expected log pointwise predictive density for a new dataset

$$\text{elpd} = \sum_{i=1}^{n} \int \log p^*(\tilde{y}_i\mid y) P_\text{T}(\mathrm{d}\tilde{y}_i)$$

- $P_\text{T}$ represents the true data-generating process for $\tilde{y}_i$
- $p^*(\cdot\mid y)$ is the denisty of the posterior predictive

## Bayes model evaluation LOO cross-validation

Approximate the elpd, using leave-one-out cross-validation:

$$\text{elpd}_\text{loo} = \sum_{i=1}^{n} \log p^*(y_i\mid y_{-i})$$
where^[Instead of fitting $n$ new posteriors, use importance sampling for $\Pi(\cdot \mid y_{-i})$. See @vehtari2017practical for details.] $p^*(y_i\mid y_{-i}) = \int p(y_i \mid \theta) \Pi(\mathrm{d}\theta \mid y_{-i})$

## References

::: {#refs}
:::

# Appendices




