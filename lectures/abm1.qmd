---
title: "Applied Bayesian Modelling"
subtitle: "Lecture 1: Bayesian statistical inference"
author: "Dr Joshua J Bon"
bibliography: ../refs.bib
format:
  revealjs:
    html-math-method: katex
    theme: default
    df-print: paged
    incremental: false 
    css: style.css
    chalkboard: true
    logo: dauphine-logo-white.png
editor: source
from: markdown+emoji
---

```{r setup}

josh_data_dir <- function(fl){
  paste0("/Users/jbon/github/mash-abm/data/", fl)
}

```


## Applied Bayesian modelling

-   Statistical inference :mag:
-   Statistical prediction
-   Bayesian models :mag:
-   Computation :mag:
-   Workflows

::: callout-note
## Links 

- [Députés 2019 data](https://github.com/bonStats/mash-abm/tree/main/data){target="_blank"}
- [Index slides](../index.html)
:::

# Statistical inference

## Example 1: Députés debut age

```{r dep-debut}
#| echo: true
#| code-fold: true

library(readr)
library(ggplot2)

deputes <- read_csv(file = josh_data_dir("deputes2019.csv"))
# Download: https://github.com/bonStats/mash-abm/tree/main/data
# change location to match folder of 'deputes2019.csv' on your computer
# e.g. "~/Downloads/deputes2019.csv" or "C:/Users/[User Name]/Downloads/deputes2019.csv"

ggplot(deputes) + 
  geom_histogram(aes(x = (premier_elu - date_naissance)/356.25)) +
  scale_x_continuous("Debut age") +
  scale_y_continuous("Number of politicians") +
  theme_bw()

```

## Statistical inference

::: callout-note
## Lay question

What age do politicians make their debut in parliament?
:::

::: callout-tip
## Statistical questions

1.  What is the *average* age at debut?
2.  What is the *variance* in age at debut?
3.  What *statistical model* best explains the distribution of question numbers?
:::

-   In France, according to the 2019 data!

# Introduction to Bayes

## Bayes rule

Two events $A$ and $B$

$$\Pr(A\mid B) = \frac{\Pr(B \mid A) \Pr(A)}{\Pr(B)}$$

## Bayes theorem for inference

-   Assume observed data: $Y \sim P(\cdot\mid\theta)$ for some unknown $\theta$
-   Assume we have a prior belief for $\theta \sim \Pi$

::: callout-tip
## Posterior distribution

$$\Pi(\mathrm{d}\theta \mid Y=y) = \frac{p(y \mid \theta)\Pi(\mathrm{d}\theta)}{m(y)}$$
:::

## Bayes theorem for inference

::: callout-tip
## Posterior density

$$\pi(\theta \mid y) = \frac{p(y \mid \theta)\pi(\theta)}{m(y)}$$
:::

-   $p(y \mid \theta)$ is density^[Probability density (e.g. PDF/PMF)] of $P(\cdot\mid\theta)$ at $y$ for a given $\theta$
-   $\pi(\theta)$ is prior density of $\theta$
-   $m(y)$ is the model evidence^[Also known as the posterior normalising constant, or marginal likelihood]

## Bayes theorem for inference

::: callout-tip
## Posterior density

$$\pi(\theta \mid y) = \frac{p(y \mid \theta)\pi(\theta)}{m(y)}$$
:::

-   Marginal likelihood
    -   $Z = m(y) = \int_\Theta p(y \mid \theta) \Pi(\mathrm{d}\theta) = \mathbb{E}_{\theta \sim \Pi}\left[p(y \mid \theta)\right]$
-   Likelihood
    -   $L(\cdot\mid y) = p(y \mid \cdot)$ for fixed $y$.

## Posterior quantities

-   Moments
-   Median, mode
-   Credible intervals (quantiles)
-   Posterior predictive distribution

## Demonstration: Posterior (Ex 1)

-   Age first elected $Y_i$ for $i\in\{1,2,\ldots,565\}$

Assume model and prior

-   $Y_i \mid \beta \overset{\text{iid}}{\sim} \text{Gamma}(\alpha,\beta)$
-   $\alpha = 200$ fixed
-   $\beta \sim \text{Gamma}(\alpha_0,\beta_0)$

# Choosing a prior

## Choice of prior: $\Pi$

-   Incorporate prior knowledge
    -   Support of parameter
    -   Moments
    -   Quantiles
    -   Prior predictive distribution
-   Hyper priors
-   Computationally tractable

## Conjugate priors

::: callout-tip
## Conjugate prior

A prior is a *conjugate prior* if given a likelihood, the prior and posterior are in the same family of distributions.
:::

For known distributions, conjugate priors are *tractable*.

## Conjugate prior examples

-   Gamma prior on rate $\beta$ is conjugate prior for Gamma likelihood (fixed shape $\alpha$)
-   Normal prior on mean $\mu$ is conjugate prior for Normal likelihood (fixed variance $\sigma^2$)
-   Inverse Gamma prior on variance $\sigma^2$ is conjugate prior for Normal likelihood (fixed mean $\mu$)

## Flat/uniform priors

$$\pi(\theta) \propto 1$$

- $\Pi$ may be an **improper prior**, $\int_{\Theta}\pi(\theta) = \infty$.

- Flat priors may still contain information[^2]

[^2]: See "The prior can often only be understood in the context of the likelihood", [@gelman2017prior]

## Non-informative and weakly-informative priors

-   Diffuse priors, e.g. $\theta \sim \text{N}(0, \sigma^2)$ with large $\sigma^2$
    -   Note: $\sigma^2 \rightarrow \infty$ is a flat improper prior
-   Priors with invariance, symmetry, uncertainty principles
    -   Jefferys priors [@robert2009harold]
    -   Reference priors [@berger2009formal]
    -   Objective priors [@ghosh2011objective]

## Jefferys priors

Recall Fisher information matrix for regular models

$$\mathcal{I}(\theta) = -\mathbb{E}\left[ \frac{\partial^2 \log L}{\partial \theta^2} \right]$$

$$\pi(\theta) \propto |\mathcal{I}(\theta)|^{1/2}$$

-   Invariance under reparameterisation
    -   Property for continuous parameters
-   May lead to improper prior

# Model choice and checking

## Choice of model

$$P(\cdot\mid\theta)$$ How are the data distributed? Is this a good approximation?

-   Incorporate data knowledge
    -   Support of data
    -   Properties of the data
        -   Groups
        -   Dynamics
    -   Prior predictive distribution

## Model checking I: Bayes Factors

$$m_i(y) = \int_\Theta p_i(y \mid \theta) \Pi_i(\mathrm{d}\theta) = \mathbb{E}_{\theta\sim\Pi_i}\left[ L_i(\theta\mid y)\right]$$

-   $m_i(y)$ is evidence for model $M_i$ for fixed $y$, depends on
    -   prior $\Pi_i$
    -   likelihood $L_i(\cdot\mid y) = p_i(y \mid \cdot)$
-   Bayes factor $\text{BF}_{1,0} = \frac{m_1(y)}{m_0(y)}$ compares evidence

## Model checking I: Bayes Factors

Interpretation of Bayes factors

$$
\text{BF}_{1,0} = \frac{\Pr(M_1\mid y)}{\Pr(M_0\mid y)} = \frac{m_1(y)}{m_0(y)}
$$

## Model checking I: Bayes Factors

Interpretation of Bayes factors

| $\text{BF}_{1,0}$ | Strength of Evidence^[@good1979studies] ($M_1$ vs null $M_0$) |
|-------------------|------------------------------------------------|
| 1 to 3.2          | Not worth more than a bare mention             |
| 3.2 to 10         | Substantial                                    |
| 10 to 100         | Strong                                         |
| $>$ 100            | Decisive                                       |

# Posterior Approximation

## Posterior Computation

What if I don't use a conjugate prior? Intractable normalising constant, unknown distribution. Why?

$$Z = \int_\Theta p(y \mid \theta) \Pi(\mathrm{d}\theta)$$

Use computational methods:

-   MCMC: Markov chain Monte Carlo
-   IS: Importance sampling
-   SMC: Sequential Monte Carlo

## Monte Carlo methods

When $\mathbb{E}_{\theta\sim P}\left[f(\theta)\right]$ is intractable...

1.  $P$ is known but expectation of $f(\theta)$ is intractable (e.g. unknown integral)
2.  $P$ is unknown due to intractable normalising constant

Could use numerical methods (quadrature), but in "high" dimension ($d \gtrsim 4$), we typically use Monte Carlo methods.

## Monte Carlo integration

::: callout-note
## Monte Carlo Approximation

$$\mathbb{E}_{\theta\sim P}\left[f(\theta)\right] \approx \frac{1}{N}\sum_{i=1}^{N}f(\theta_i), \quad \text{where}~\theta_i \overset{\text{iid}}{\sim} P$$
:::

In the sense that if $\mathbb{E}_{\theta\sim P}\left[f(\theta)\right]  < \infty$ then

$$\frac{1}{N}\sum_{i=1}^{N}f(\theta_i) \overset{\text{P}}{\longrightarrow} \mathbb{E}_{\theta\sim P}\left[f(\theta)\right],\quad n\rightarrow \infty$$

## Monte Carlo CLT

$$\sqrt{N} \left[ \frac{1}{N}\sum_{i=1}^{N}f(\theta_i) - \mathbb{E}_{\theta\sim P}\left[f(\theta)\right]\right] \overset{\text{d}}{\longrightarrow} N(0, \sigma^2_f)$$
if $\theta_i \overset{\text{iid}}{\sim} P$, and $\sigma^2_f = \text{Var}[f(\theta_i)] < \infty$.

- Vanilla Monte Carlo error has rate $\frac{\sigma_f}{\sqrt{N}} = \mathcal{O}(N^{-1/2})$

## Importance sampling

::: callout-note
## Importance sampling approximation

$$\mathbb{E}_{\theta\sim P}\left[f(\theta)\right] = \mathbb{E}_{\theta\sim Q}\left[f(\theta)\frac{p(\theta)}{q(\theta)}\right] \approx \frac{1}{N}\sum_{i=1}^{N}f(\theta_i)\frac{p(\theta_i)}{q(\theta_i)}, \quad \text{where}~\theta_i \overset{\text{iid}}{\sim} Q$$
:::

In the sense that if $\mathbb{E}_{\theta\sim P}\left[f(\theta)\right]  < \infty$ then

$$\frac{1}{N}\sum_{i=1}^{N}f(\theta_i)\frac{p(\theta_i)}{q(\theta_i)} \overset{\text{P}}{\longrightarrow} \mathbb{E}_{\theta\sim P}\left[f(\theta)\right],\quad n\rightarrow \infty$$

## Approximating model evidence

Vanilla Monte Carlo estimator

$$m(y) = \mathbb{E}_{\theta \sim \Pi}\left[ L(\theta \mid y) \right]$$
Importance sample estimator

$$m(y) = \mathbb{E}_{\theta \sim Q}\left[ L(\theta \mid y) \frac{p(\theta)}{q(\theta)}\right]$$

Choose $q(\theta) \approx C \cdot L(\theta \mid y) p(\theta)$

## Recap: Ingredients of Bayesian models

1.  Posterior = prior $\times$ likelihood
2.  Bayesian model = $\{$ prior, data model $\}$
3.  Model evidence
4.  Intractable posteriors $\rightarrow$ Monte Carlo approximation

## Example 2: Députés Questions

```{r dep-questions}
#| echo: true
#| code-fold: true

library(readr)
library(ggplot2)

deputes <- read_csv(file = josh_data_dir("deputes2019.csv"))
# Download: https://github.com/bonStats/mash-abm/tree/main/data
# change location to match folder of 'deputes2019.csv' on your computer
# e.g. "~/Downloads/deputes2019.csv" or "C:/Users/[User Name]/Downloads/deputes2019.csv"

ggplot(deputes) + 
  geom_histogram(aes(x = questions_orales)) +
  facet_grid(rows = "sexe") + 
  theme_bw()

```

## Example 2: Députés Questions

::: callout-note
## Lay question

Do the number of questions politicians ask differ by gender?
:::

::: callout-tip
## Statistical questions

1.  Does the *average* number of questions differ by gender?
2.  Does the *variance* in the number of questions differ by gender?
3.  What *statistical model* best explains the distribution of question numbers?
:::

## References

::: {#refs}
:::

# Appendices
