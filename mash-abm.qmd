---
title: "Applied Bayesian Modelling"
subtitle: "MASH M2"
author: "Dr Joshua J Bon"
bibliography: refs.bib
format:
  revealjs:
    html-math-method: katex
    theme: default
    df-print: paged
    incremental: false 
    css: style.css
    chalkboard: true
    logo: dauphine-logo-white.png
editor: source
---


## Getting started

Applied Bayesian modelling
 - Statistical inference, prediction
 - Bayesian models
 - Computation
 - Workflows


# Section 1: Bayesian statistics

## Example 1: Députés debut age

```{r dep-debut}
#| echo: true
#| code-fold: true

library(readr)
library(ggplot2)

deputes <- read_csv(file = "data/deputes2019.csv") 
# change location to match folder of 'deputes2019.csv' on your computer

ggplot(deputes) + 
  geom_histogram(aes(x = (premier_elu - date_naissance)/356.25)) +
  scale_x_continuous("Debut age") +
  scale_y_continuous("Number of politicians") +
  theme_bw()

```

## Statistical inference and prediction

:::{.callout-note}
## Lay question 
What age do politicians make their debut in parliament?
:::

:::{.callout-tip}
## Statistical questions

1. What is the _average_ age at debut?
2. What is the _variance_ in age at debut?
3. What _statistical model_ best explains the distribution of question numbers? 
:::

- In France, according to the 2019 data!

## Bayes rule

Two events $A$ and $B$

$$\Pr(A\mid B) = \frac{\Pr(B \mid A) \Pr(A)}{\Pr(B)}$$

## Bayes theorem for inference

- Assume observed data: $X \sim P_\theta$ for some unknown $\theta$.
- Assume we have a prior belief for $\theta \sim \Pi$ 

:::{.callout-tip}
## Posterior distribution

$$\Pi(\mathrm{d}\theta \mid X=x) = \frac{p(x \mid \theta)\Pi(\mathrm{d}\theta)}{p(x)}$$

:::

## Bayes theorem for inference

:::{.callout-tip}
## Posterior density
$$\pi(\theta \mid x) = \frac{p(x \mid \theta)\pi(\theta)}{Z}$$
:::
-  $p(x \mid \theta)$ is density^[Probability density (mass) function (PDF/PMF).] of $P_\theta$ at $x$ for a given $\theta$.
- $\pi(\theta)$ is prior density of $\theta$.
- $Z$ is normalising constant (or model evidence)

## Bayes theorem for inference

:::{.callout-tip}
## Posterior density
$$\pi(\theta \mid x) = \frac{p(x \mid \theta)\pi(\theta)}{Z}$$
:::

- Marginal likelihood
  - $Z = p(x) = \int_\Theta p(x \mid \theta) \Pi(\mathrm{d}\theta)$
- Likelihood
  - $L(\theta) = p(x \mid \theta)$ for fixed $x$.
  
## Posterior quantities

- Moments
- Median, mode
- Credible intervals (quantiles)
- Posterior predictive distribution

## Demonstration: Posterior (Ex 1) 

- Age first elected $X_i$ for $i\in\{1,2,\ldots,565\}$

Assume model and prior

- $X_i \mid \beta \overset{\text{iid}}{\sim} \text{Gamma}(\alpha,\beta)$
- $\alpha = 200$ fixed
- $\beta \sim \text{Gamma}(\alpha_0,\beta_0)$


## Conjugate priors

A prior is a _conjugate prior_ if given a likelihood, the prior and posterior are in the same family of distributions.

For known distributions, conjugate priors are _tractable_.

- Gamma prior on rate is conjugate prior for Gamma likelihood (fixed scale)
- Normal prior on mean is conjugate prior for Normal likelihood (fixed variance)
- Inverse Gamma prior on variance is conjugate prior for Normal likelihood (fixed mean)

## Jefferys prior

Recall Fisher information matrix

$$\mathcal{I}(\theta) = \mathbb{E}\left[ \left(\frac{\partial \log L}{\partial \theta}\right)^2 \right]$$

or for regular models $\mathcal{I}(\theta) = -\mathbb{E}\left[ \frac{\partial^2 \log L}{\partial \theta^2} \right]$

$$\pi(\theta) \propto \$$

## Posterior Computation

What if I don't use a conjugate prior? Intractable normalising constant. Why?

$$Z = \int_\Theta p(x \mid \theta) \Pi(\mathrm{d}\theta)$$

Use computational methods:

- MCMC: Markov chain Monte Carlo
- IS: Importance sampling
- SMC: Sequential Monte Carlo

## Choice of prior

$$\Pi(\mathrm{d}\theta)$$

- Incorporate prior knowledge
  - Support of parameter
  - Moments
  - Quantiles
  - Prior predictive distribution
- Hyper priors
- Computationally tractable

## Choice of model

$$P_\theta$$
How are the data distributed? Is this a good approximation?

- Incorporate data knowledge
  - Support of data
  - Properties of the data
    - Groups
    - Dynamics
  - Prior predictive distribution
  
## Model checking I: Model evidence




## Recap: Ingredients of Bayesian models

1. Prior
2. Model
3. Computation

## Example 2: Députés Questions

```{r dep-questions}
#| echo: true
#| code-fold: true

library(readr)
library(ggplot2)

deputes <- read_csv(file = "data/deputes2019.csv") 
# change location to match folder of 'deputes2019.csv' on your computer

ggplot(deputes) + 
  geom_histogram(aes(x = questions_orales)) +
  facet_grid(rows = "sexe") + 
  theme_bw()

```

## Example 2: Statistical inference

:::{.callout-note}
## Lay question 
Do the number of questions politicians ask differ by gender?
:::

:::{.callout-tip}
## Statistical questions

1. Does the _average_ number of questions differ by gender?
2. Does the _variance_ in the number of questions differ by gender?
3. What _statistical model_ best explains the distribution of question numbers? 
:::


# Section 2: Bayes computation

## Monte Carlo

## Importance sampling

## Markov chain Monte Carlo

# Section 3: 

# Section 4

# Section 5

## References

::: {#refs}
:::

# Appendices