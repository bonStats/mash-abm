[
  {
    "objectID": "lectures/abm1.html#applied-bayesian-modelling",
    "href": "lectures/abm1.html#applied-bayesian-modelling",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Applied Bayesian modelling",
    "text": "Applied Bayesian modelling\n\nStatistical inference üîç\nStatistical prediction\nBayesian models üîç\nComputation üîç\nWorkflows"
  },
  {
    "objectID": "lectures/abm1.html#example-1-d√©put√©s-debut-age",
    "href": "lectures/abm1.html#example-1-d√©put√©s-debut-age",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Example 1: D√©put√©s debut age",
    "text": "Example 1: D√©put√©s debut age\n\n\nCode\nlibrary(readr)\nlibrary(ggplot2)\n\ndeputes &lt;- read_csv(file = josh_data_dir(\"deputes2019.csv\"))\n# Download: https://github.com/bonStats/mash-abm/tree/main/data\n# change location to match folder of 'deputes2019.csv' on your computer\n# e.g. \"~/Downloads/deputes2019.csv\" or \"C:/Users/[User Name]/Downloads/deputes2019.csv\"\n\nggplot(deputes) + \n  geom_histogram(aes(x = (premier_elu - date_naissance)/365.25)) +\n  scale_x_continuous(\"Debut age\") +\n  scale_y_continuous(\"Number of politicians\") +\n  theme_bw()"
  },
  {
    "objectID": "lectures/abm1.html#statistical-inference-1",
    "href": "lectures/abm1.html#statistical-inference-1",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Statistical inference",
    "text": "Statistical inference\n\n\n\n\n\n\nLay question\n\n\nWhat age do politicians make their debut in parliament?\n\n\n\n\n\n\n\n\n\nStatistical questions\n\n\n\nWhat is the average age at debut?\nWhat is the variance in age at debut?\nWhat statistical model best explains the distribution of age?\n\n\n\n\n\nIn France, according to the 2019 data!"
  },
  {
    "objectID": "lectures/abm1.html#bayes-rule",
    "href": "lectures/abm1.html#bayes-rule",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Bayes rule",
    "text": "Bayes rule\nTwo events A and B\n\\Pr(A\\mid B) = \\frac{\\Pr(B \\mid A) \\Pr(A)}{\\Pr(B)}"
  },
  {
    "objectID": "lectures/abm1.html#bayes-theorem-for-inference",
    "href": "lectures/abm1.html#bayes-theorem-for-inference",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Bayes theorem for inference",
    "text": "Bayes theorem for inference\n\nAssume observed data: Y \\sim P(\\cdot\\mid\\theta) for some unknown \\theta\nAssume we have a prior belief for \\theta \\sim \\Pi\n\n\n\n\n\n\n\nPosterior distribution\n\n\n\\Pi(\\mathrm{d}\\theta \\mid Y=y) = \\frac{p(y \\mid \\theta)\\Pi(\\mathrm{d}\\theta)}{m(y)}"
  },
  {
    "objectID": "lectures/abm1.html#bayes-theorem-for-inference-1",
    "href": "lectures/abm1.html#bayes-theorem-for-inference-1",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Bayes theorem for inference",
    "text": "Bayes theorem for inference\n\n\n\n\n\n\nPosterior density\n\n\n\\pi(\\theta \\mid y) = \\frac{p(y \\mid \\theta)\\pi(\\theta)}{m(y)}\n\n\n\n\np(y \\mid \\theta) is density1 of P(\\cdot\\mid\\theta) at y for a given \\theta\n\\pi(\\theta) is prior density of \\theta\nm(y) is the model evidence2\n\nProbability density (e.g.¬†PDF/PMF)Also known as the posterior normalising constant, or marginal likelihood"
  },
  {
    "objectID": "lectures/abm1.html#bayes-theorem-for-inference-2",
    "href": "lectures/abm1.html#bayes-theorem-for-inference-2",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Bayes theorem for inference",
    "text": "Bayes theorem for inference\n\n\n\n\n\n\nPosterior density\n\n\n\\pi(\\theta \\mid y) = \\frac{p(y \\mid \\theta)\\pi(\\theta)}{m(y)}\n\n\n\n\nLikelihood\n\nL(\\cdot\\mid y) = p(y \\mid \\cdot) for fixed y.\n\nMarginal likelihood\n\nm(y) = \\int_\\Theta p(y \\mid \\theta) \\Pi(\\mathrm{d}\\theta) = \\mathbb{E}_{\\theta \\sim \\Pi}\\left[L(\\theta\\mid y)\\right]"
  },
  {
    "objectID": "lectures/abm1.html#posterior-quantities",
    "href": "lectures/abm1.html#posterior-quantities",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Posterior quantities",
    "text": "Posterior quantities\n\nMoments\nMedian, mode\nPosterior quantiles\nCredible intervals\nPosterior predictive distribution"
  },
  {
    "objectID": "lectures/abm1.html#demonstration-posterior-ex-1",
    "href": "lectures/abm1.html#demonstration-posterior-ex-1",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Demonstration: Posterior (Ex 1)",
    "text": "Demonstration: Posterior (Ex 1)\n\nAge first elected Y_i for i\\in\\{1,2,\\ldots,565\\}\n\nAssume model and prior\n\nY_i \\mid \\beta \\overset{\\text{iid}}{\\sim} \\text{Gamma}(\\alpha,\\beta)\n\\alpha = 200 fixed\n\\beta \\sim \\text{Gamma}(\\alpha_0,\\beta_0)"
  },
  {
    "objectID": "lectures/abm1.html#choice-of-prior-pi",
    "href": "lectures/abm1.html#choice-of-prior-pi",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Choice of prior: \\Pi",
    "text": "Choice of prior: \\Pi\n\nIncorporate prior knowledge\n\nSupport of parameter\nMoments\nQuantiles\nPrior predictive distribution\n\nHyper priors\nComputationally tractable"
  },
  {
    "objectID": "lectures/abm1.html#conjugate-priors",
    "href": "lectures/abm1.html#conjugate-priors",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Conjugate priors",
    "text": "Conjugate priors\n\n\n\n\n\n\nConjugate prior\n\n\nA prior is a conjugate prior if given a likelihood, the prior and posterior are in the same family of distributions.\n\n\n\nFor known distributions, conjugate priors lead to tractable posterior distributions."
  },
  {
    "objectID": "lectures/abm1.html#conjugate-prior-examples",
    "href": "lectures/abm1.html#conjugate-prior-examples",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Conjugate prior examples",
    "text": "Conjugate prior examples\n\nGamma prior on rate \\beta is conjugate prior for Gamma likelihood (fixed shape \\alpha)\nNormal prior on mean \\mu is conjugate prior for Normal likelihood (fixed variance \\sigma^2)\nInverse Gamma prior on variance \\sigma^2 is conjugate prior for Normal likelihood (fixed mean \\mu)"
  },
  {
    "objectID": "lectures/abm1.html#flatuniform-priors",
    "href": "lectures/abm1.html#flatuniform-priors",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Flat/uniform priors",
    "text": "Flat/uniform priors\n\\pi(\\theta) \\propto 1\n\n\\Pi may be an improper prior, \\int_{\\Theta}\\pi(\\theta) = \\infty.\nFlat priors may still contain information1\n\nSee ‚ÄúThe prior can often only be understood in the context of the likelihood‚Äù, (Gelman, Simpson, and Betancourt 2017)"
  },
  {
    "objectID": "lectures/abm1.html#non-informative-and-weakly-informative-priors",
    "href": "lectures/abm1.html#non-informative-and-weakly-informative-priors",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Non-informative and weakly-informative priors",
    "text": "Non-informative and weakly-informative priors\n\nDiffuse priors, e.g.¬†\\theta \\sim \\text{N}(0, \\sigma^2) with large \\sigma^2\n\nNote: \\sigma^2 \\rightarrow \\infty is a flat improper prior\n\nPriors with invariance, symmetry, uncertainty principles\n\nJefferys priors (Robert, Chopin, and Rousseau 2009)\nReference priors (Berger, Bernardo, and Sun 2009)\nObjective priors (Ghosh 2011)"
  },
  {
    "objectID": "lectures/abm1.html#jefferys-priors",
    "href": "lectures/abm1.html#jefferys-priors",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Jefferys priors",
    "text": "Jefferys priors\nRecall Fisher information matrix for regular models\n\\mathcal{I}(\\theta) = -\\mathbb{E}\\left[ \\frac{\\partial^2 \\log L}{\\partial \\theta^2} \\right]\n\\pi(\\theta) \\propto |\\mathcal{I}(\\theta)|^{1/2}\n\nInvariance under reparameterisation\n\nProperty for continuous parameters\n\nMay lead to improper prior"
  },
  {
    "objectID": "lectures/abm1.html#choice-of-model",
    "href": "lectures/abm1.html#choice-of-model",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Choice of model",
    "text": "Choice of model\nP(\\cdot\\mid\\theta) How are the data distributed? Is this a good approximation?\n\nIncorporate data knowledge\n\nSupport of data\nProperties of the data\n\nGroups\nDynamics\n\nPrior predictive distribution"
  },
  {
    "objectID": "lectures/abm1.html#model-checking-i-bayes-factors",
    "href": "lectures/abm1.html#model-checking-i-bayes-factors",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Model checking I: Bayes Factors",
    "text": "Model checking I: Bayes Factors\nm_i(y) = \\mathbb{E}_{\\theta\\sim\\Pi_i}\\left[ L_i(\\theta\\mid y)\\right] = \\int_{\\Theta_i} p_i(y \\mid \\theta_i) \\Pi_i(\\mathrm{d}\\theta_i)\n\nm_i(y) is evidence for model \\mathcal{M}_i for fixed y, depends on\n\nprior \\Pi_i\nlikelihood L_i(\\cdot\\mid y) = p_i(y \\mid \\cdot)\n\nBayes factor \\text{BF}_{10} = \\frac{m_1(y)}{m_0(y)} compares evidence"
  },
  {
    "objectID": "lectures/abm1.html#model-checking-i-bayes-factors-1",
    "href": "lectures/abm1.html#model-checking-i-bayes-factors-1",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Model checking I: Bayes Factors",
    "text": "Model checking I: Bayes Factors\nInterpretation of Bayes factors\n\n\\text{BF}_{1,0}  = \\frac{m_1(y)}{m_0(y)} = \\frac{\\Pr(M_1\\mid y)}{\\Pr(M_0\\mid y)}"
  },
  {
    "objectID": "lectures/abm1.html#model-checking-i-bayes-factors-2",
    "href": "lectures/abm1.html#model-checking-i-bayes-factors-2",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Model checking I: Bayes Factors",
    "text": "Model checking I: Bayes Factors\nInterpretation of Bayes factors (Jeffreys 1998)\n\n\n\n\n\n\n\n\\text{BF}_{10}\nStrength of Evidence\n\n\n\n\n&lt;1\nNegative (supports M_0)\n\n\n1 to 10^{1/2}\nBarely worth mentioning\n\n\n10^{1/2} to 10\nSubstantial\n\n\n10 to 10^{3/2}\nStrong\n\n\n10^{3/2} to 100\nVery strong\n\n\n&gt; 100\nDecisive"
  },
  {
    "objectID": "lectures/abm1.html#model-checking-i-bayes-factors-3",
    "href": "lectures/abm1.html#model-checking-i-bayes-factors-3",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Model checking I: Bayes Factors",
    "text": "Model checking I: Bayes Factors\nInterpretation of Bayes factors (Kass and Raftery 1995)\n\n\n\n\\text{BF}_{10}\nStrength of Evidence (M_1 vs null M_0)\n\n\n\n\n1 to 3.2\nNot worth more than a bare mention\n\n\n3.2 to 10\nSubstantial\n\n\n10 to 100\nStrong\n\n\n&gt; 100\nDecisive"
  },
  {
    "objectID": "lectures/abm1.html#example-2-d√©put√©s-questions",
    "href": "lectures/abm1.html#example-2-d√©put√©s-questions",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Example 2: D√©put√©s Questions",
    "text": "Example 2: D√©put√©s Questions\n\n\nCode\nlibrary(readr)\nlibrary(ggplot2)\n\ndeputes &lt;- read_csv(file = josh_data_dir(\"deputes2019.csv\"))\n# Download: https://github.com/bonStats/mash-abm/tree/main/data\n# change location to match folder of 'deputes2019.csv' on your computer\n# e.g. \"~/Downloads/deputes2019.csv\" or \"C:/Users/[User Name]/Downloads/deputes2019.csv\"\n\nggplot(deputes) + \n  geom_histogram(aes(x = questions_orales)) +\n  facet_grid(rows = \"sexe\") + \n  theme_bw()"
  },
  {
    "objectID": "lectures/abm1.html#example-2-d√©put√©s-questions-1",
    "href": "lectures/abm1.html#example-2-d√©put√©s-questions-1",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Example 2: D√©put√©s Questions",
    "text": "Example 2: D√©put√©s Questions\n\n\n\n\n\n\nLay question\n\n\nDo the number of questions politicians ask differ by gender?\n\n\n\n\n\n\n\n\n\nStatistical questions\n\n\n\nDoes the average number of questions differ by gender?\nDoes the variance in the number of questions differ by gender?\nWhat statistical model best explains the distribution of question numbers?"
  },
  {
    "objectID": "lectures/abm1.html#posterior-computation",
    "href": "lectures/abm1.html#posterior-computation",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Posterior Computation",
    "text": "Posterior Computation\nWhat if I don‚Äôt use a conjugate prior? Intractable normalising constant, unknown distribution. Why?\nZ = \\int_\\Theta p(y \\mid \\theta) \\Pi(\\mathrm{d}\\theta)\nUse computational methods:\n\nMCMC: Markov chain Monte Carlo\nIS: Importance sampling\nSMC: Sequential Monte Carlo"
  },
  {
    "objectID": "lectures/abm1.html#monte-carlo-methods",
    "href": "lectures/abm1.html#monte-carlo-methods",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Monte Carlo methods",
    "text": "Monte Carlo methods\nWhen \\mathbb{E}_{\\theta\\sim P}\\left[f(\\theta)\\right] is intractable‚Ä¶\n\nP is known but expectation of f(\\theta) is intractable (e.g.¬†unknown integral)\nP is unknown due to intractable normalising constant\n\nCould use numerical methods (quadrature), but in ‚Äúhigh‚Äù dimension (d \\gtrsim 4), we typically use Monte Carlo methods."
  },
  {
    "objectID": "lectures/abm1.html#monte-carlo-integration",
    "href": "lectures/abm1.html#monte-carlo-integration",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Monte Carlo integration",
    "text": "Monte Carlo integration\n\n\n\n\n\n\nMonte Carlo Approximation\n\n\n\\mathbb{E}_{\\theta\\sim P}\\left[f(\\theta)\\right] \\approx \\frac{1}{N}\\sum_{i=1}^{N}f(\\theta_i), \\quad \\text{where}~\\theta_i \\overset{\\text{iid}}{\\sim} P\n\n\n\nIn the sense that if \\mathbb{E}_{\\theta\\sim P}\\left[f(\\theta)\\right]  &lt; \\infty then\n\\frac{1}{N}\\sum_{i=1}^{N}f(\\theta_i) \\overset{\\text{P}}{\\longrightarrow} \\mathbb{E}_{\\theta\\sim P}\\left[f(\\theta)\\right],\\quad n\\rightarrow \\infty"
  },
  {
    "objectID": "lectures/abm1.html#monte-carlo-clt",
    "href": "lectures/abm1.html#monte-carlo-clt",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Monte Carlo CLT",
    "text": "Monte Carlo CLT\n\\sqrt{N} \\left[ \\frac{1}{N}\\sum_{i=1}^{N}f(\\theta_i) - \\mathbb{E}_{\\theta\\sim P}\\left[f(\\theta)\\right]\\right] \\overset{\\text{d}}{\\longrightarrow} N(0, \\sigma^2_f) if \\theta_i \\overset{\\text{iid}}{\\sim} P, and \\sigma^2_f = \\text{Var}[f(\\theta_i)] &lt; \\infty.\n\nVanilla Monte Carlo error has rate \\frac{\\sigma_f}{\\sqrt{N}} = \\mathcal{O}(N^{-1/2})"
  },
  {
    "objectID": "lectures/abm1.html#importance-sampling",
    "href": "lectures/abm1.html#importance-sampling",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Importance sampling",
    "text": "Importance sampling\n\n\n\n\n\n\nImportance sampling approximation\n\n\n\\mathbb{E}_{\\theta\\sim P}\\left[f(\\theta)\\right] = \\mathbb{E}_{\\theta\\sim Q}\\left[f(\\theta)\\frac{p(\\theta)}{q(\\theta)}\\right] \\approx \\frac{1}{N}\\sum_{i=1}^{N}f(\\theta_i)\\frac{p(\\theta_i)}{q(\\theta_i)}, \\quad \\text{where}~\\theta_i \\overset{\\text{iid}}{\\sim} Q\n\n\n\nIn the sense that if \\mathbb{E}_{\\theta\\sim P}\\left[f(\\theta)\\right]  &lt; \\infty then\n\\frac{1}{N}\\sum_{i=1}^{N}f(\\theta_i)\\frac{p(\\theta_i)}{q(\\theta_i)} \\overset{\\text{P}}{\\longrightarrow} \\mathbb{E}_{\\theta\\sim P}\\left[f(\\theta)\\right],\\quad n\\rightarrow \\infty"
  },
  {
    "objectID": "lectures/abm1.html#approximating-model-evidence",
    "href": "lectures/abm1.html#approximating-model-evidence",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Approximating model evidence",
    "text": "Approximating model evidence\nVanilla Monte Carlo estimator\nm(y) = \\mathbb{E}_{\\theta \\sim \\Pi}\\left[ L(\\theta \\mid y) \\right] Importance sample estimator\nm(y) = \\mathbb{E}_{\\theta \\sim Q}\\left[ L(\\theta \\mid y) \\frac{p(\\theta)}{q(\\theta)}\\right]\nChoose q(\\theta) \\approx C \\cdot L(\\theta \\mid y) p(\\theta)"
  },
  {
    "objectID": "lectures/abm1.html#recap-ingredients-of-bayesian-models",
    "href": "lectures/abm1.html#recap-ingredients-of-bayesian-models",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "Recap: Ingredients of Bayesian models",
    "text": "Recap: Ingredients of Bayesian models\n\nPosterior = prior \\times likelihood\nBayesian model = \\{ prior, data model \\}\nModel evidence\nIntractable posteriors \\rightarrow Monte Carlo approximation"
  },
  {
    "objectID": "lectures/abm1.html#references",
    "href": "lectures/abm1.html#references",
    "title": "Bayesian modelling & Monte Carlo",
    "section": "References",
    "text": "References\n\n\nBerger, James O, Jos√© M Bernardo, and Dongchu Sun. 2009. ‚ÄúThe Formal Definition of Reference Priors.‚Äù The Annals of Statistics 37 (2): 905‚Äì38.\n\n\nGelman, Andrew, Daniel Simpson, and Michael Betancourt. 2017. ‚ÄúThe Prior Can Often Only Be Understood in the Context of the Likelihood.‚Äù Entropy 19 (10): 555.\n\n\nGhosh, Malay. 2011. ‚ÄúObjective Priors: An Introduction for Frequentists.‚Äù Statistical Science, 187‚Äì202.\n\n\nJeffreys, H. 1998. The Theory of Probability. Oxford Classic Texts in the Physical Sciences. OUP Oxford. https://books.google.fr/books?id=vh9Act9rtzQC.\n\n\nKass, Robert E, and Adrian E Raftery. 1995. ‚ÄúBayes Factors.‚Äù Journal of the American Statistical Association 90 (430): 773‚Äì95.\n\n\nRobert, Christian P, Nicolas Chopin, and Judith Rousseau. 2009. ‚ÄúHarold Jeffreys‚Äôs Theory of Probability Revisited.‚Äù Statistical Science, 141‚Äì72."
  },
  {
    "objectID": "lectures/abm3.html#applied-bayesian-modelling",
    "href": "lectures/abm3.html#applied-bayesian-modelling",
    "title": "Gibbs samplers and variable selection",
    "section": "Applied Bayesian modelling",
    "text": "Applied Bayesian modelling\n\nStatistical inference üîç\nStatistical prediction üîç\nBayesian models\nComputation üîç\nWorkflows"
  },
  {
    "objectID": "lectures/abm3.html#example-3-explaining-death-rates",
    "href": "lectures/abm3.html#example-3-explaining-death-rates",
    "title": "Gibbs samplers and variable selection",
    "section": "Example 3: Explaining death rates",
    "text": "Example 3: Explaining death rates\n\n\nCode\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(GGally)\n\ndeathrate &lt;- read_csv(file = josh_data_dir(\"deathrate2.csv\"), col_types = cols(col_skip(), col_guess()))\n# skip the first column\n# Download: https://github.com/bonStats/mash-abm/tree/main/data\n# change location to match folder of 'deathrate2.csvs' on your computer\n# e.g. \"~/Downloads/deathrate2.csv\" or \"C:/Users/[User Name]/Downloads/deathrate2.csv\"\n\n#deathrate \n\ndeathrate %&gt;% \n  select(1:8) %&gt;%\n  ggpairs(progress = F)"
  },
  {
    "objectID": "lectures/abm3.html#example-3-explaining-death-rates-1",
    "href": "lectures/abm3.html#example-3-explaining-death-rates-1",
    "title": "Gibbs samplers and variable selection",
    "section": "Example 3: Explaining death rates",
    "text": "Example 3: Explaining death rates\n\n\nCode\ndeathrate %&gt;% \n  select(c(1,9:16)) %&gt;%\n  ggpairs(progress = F)"
  },
  {
    "objectID": "lectures/abm3.html#example-3-explaining-death-rates-2",
    "href": "lectures/abm3.html#example-3-explaining-death-rates-2",
    "title": "Gibbs samplers and variable selection",
    "section": "Example 3: Explaining death rates",
    "text": "Example 3: Explaining death rates\n\n\nCode\ndeathrate %&gt;% \n  select(c(1,starts_with(\"noise\"))) %&gt;%\n  ggpairs(progress = F)"
  },
  {
    "objectID": "lectures/abm3.html#linear-regression",
    "href": "lectures/abm3.html#linear-regression",
    "title": "Gibbs samplers and variable selection",
    "section": "Linear regression",
    "text": "Linear regression\ny = X \\beta + \\epsilon y \\in \\mathbb{R}^n, \\quad \\beta \\in \\mathbb{R}^{p+1}, \\quad \\epsilon_i \\overset{\\text{iid}}{\\sim} \\mathcal{N}(0, \\sigma^2)\nX = \\begin{bmatrix}\n  \\mid  & \\mid & \\mid & &\\mid \\\\\n  1  & x_1 & x_2 & \\cdots & x_p \\\\\n  \\mid  & \\mid & \\mid & &\\mid\n  \\end{bmatrix}\nTypically n \\gg p, and no spurious variables y‚Ä¶\nwhat to do if this is not the case?"
  },
  {
    "objectID": "lectures/abm3.html#variable-selection",
    "href": "lectures/abm3.html#variable-selection",
    "title": "Gibbs samplers and variable selection",
    "section": "Variable selection",
    "text": "Variable selection\n\nForward/backward selection\n\nHypothesis testing (Frequentist)\nBayes factors\n\nBayes model averaging\nMarginal posterior inclusion probabilities"
  },
  {
    "objectID": "lectures/abm3.html#posterior-predictive-1",
    "href": "lectures/abm3.html#posterior-predictive-1",
    "title": "Gibbs samplers and variable selection",
    "section": "Posterior predictive",
    "text": "Posterior predictive\nIf we have data model P(\\cdot\\mid\\theta) and posterior \\Pi(\\cdot\\mid y),\nthen the posterior predictive distribution is\nP^\\ast (\\cdot \\mid y) = \\int_{\\Theta}P(\\cdot\\mid\\theta) \\Pi( \\mathrm{d}\\theta \\mid y)"
  },
  {
    "objectID": "lectures/abm3.html#posterior-predictive-samples",
    "href": "lectures/abm3.html#posterior-predictive-samples",
    "title": "Gibbs samplers and variable selection",
    "section": "Posterior predictive samples",
    "text": "Posterior predictive samples\n\nSample \\theta_i \\sim \\Pi( \\cdot \\mid y)\nSample y^\\ast_i \\sim P(\\cdot\\mid \\theta_i)\n\nMarginal distribution y^\\ast_i \\mid y is posterior predictive,\ny^\\ast_i \\sim P^\\ast (\\cdot\\mid y)"
  },
  {
    "objectID": "lectures/abm3.html#gibb-samplers",
    "href": "lectures/abm3.html#gibb-samplers",
    "title": "Gibbs samplers and variable selection",
    "section": "Gibb samplers",
    "text": "Gibb samplers\n\nDivide-and-conquer\n\nHigh-dim sampling problem \\Rightarrow several low-dim problems\n\nRely on sampling conditional posterior distributions\n\nBuild invariant transition kernels for easier problems\nUse exact conditional or conditional MH step"
  },
  {
    "objectID": "lectures/abm3.html#gibb-samplers-1",
    "href": "lectures/abm3.html#gibb-samplers-1",
    "title": "Gibbs samplers and variable selection",
    "section": "Gibb samplers",
    "text": "Gibb samplers\nWhy?\n\nConditional distributions may have known analytical forms\n\nCan perform exact sampling from conditionals\nno tuning parameters!\n\nConditional distributions lower dimensional\n\nMetropolis-Hastings can be very efficient\nMH easier to tune in low dimension"
  },
  {
    "objectID": "lectures/abm3.html#posterior-conditionals",
    "href": "lectures/abm3.html#posterior-conditionals",
    "title": "Gibbs samplers and variable selection",
    "section": "Posterior conditionals",
    "text": "Posterior conditionals\nLet \\dim (\\theta) = d with posterior density \\pi(\\theta \\mid y)\nSeparate elements of \\theta using indices partition\nS_1 \\cup S_2 \\cup \\cdots \\cup S_k = \\{1,2,\\ldots,d\\}, 2 \\leq k \\leq d\nConditional distributions are\n\\begin{matrix}\\pi(\\theta^{S_1}\\mid y, \\theta^{\\bar{S_1}}) \\\\ \\pi(\\theta^{S_2}\\mid y, \\theta^{\\bar{S_2}}) \\\\ \\vdots \\\\ \\pi(\\theta^{S_k}\\mid y, \\theta^{\\bar{S_k}})   \\end{matrix}"
  },
  {
    "objectID": "lectures/abm3.html#posterior-full-conditionals",
    "href": "lectures/abm3.html#posterior-full-conditionals",
    "title": "Gibbs samplers and variable selection",
    "section": "Posterior full conditionals",
    "text": "Posterior full conditionals\nIf S_i=\\{i\\} the conditional distributions are\n\\pi(\\theta^{\\{i\\}}\\mid y, \\theta^{\\overline{\\{i\\}}}) = \\pi(\\theta^{i}\\mid y, \\theta^{-i})\nFor example with k=3, one possible ordering is\n\\begin{matrix}\\pi(\\theta^{\\{1\\}}\\mid y, \\theta^{\\{2,3\\}}) \\\\ \\pi(\\theta^{\\{2\\}}\\mid y, \\theta^{\\{1,3\\}}) \\\\ \\vdots \\\\ \\pi(\\theta^{\\{3\\}}\\mid y, \\theta^{\\{1,2\\}})   \\end{matrix}"
  },
  {
    "objectID": "lectures/abm3.html#gibb-sampling-algorithm",
    "href": "lectures/abm3.html#gibb-sampling-algorithm",
    "title": "Gibbs samplers and variable selection",
    "section": "Gibb sampling algorithm",
    "text": "Gibb sampling algorithm\nInitialise \\theta_0. At time t:\n\nSet \\vartheta = \\theta_{t-1}\nFor i \\in \\{1,2\\ldots,k\\}\n\n\nSample conditional posterior \\theta_t^{S_i} \\sim \\Pi(\\cdot\\mid y, \\vartheta^{\\bar{S}_i})\nUpdate \\vartheta^{S_i} = \\theta_t^{S_i}\n\nRun until convergence (check diagnostics)"
  },
  {
    "objectID": "lectures/abm3.html#gibb-sampling-algorithm-ex",
    "href": "lectures/abm3.html#gibb-sampling-algorithm-ex",
    "title": "Gibbs samplers and variable selection",
    "section": "Gibb sampling algorithm (Ex)",
    "text": "Gibb sampling algorithm (Ex)\nConsider posterior distribution, conditional on summary statistics s and t (and number of data points n).\n\\pi(\\alpha,\\beta\\mid s,t) \\propto {n\\choose \\alpha} \\beta^{\\alpha+s-1}(1-\\beta)^{n-\\alpha+t-1}\nfor \\alpha \\in \\{0,1,\\ldots,n\\} and \\beta \\in [0,1]\nCasella and George (1992)"
  },
  {
    "objectID": "lectures/abm3.html#gibb-sampling-algorithm-ex-1",
    "href": "lectures/abm3.html#gibb-sampling-algorithm-ex-1",
    "title": "Gibbs samplers and variable selection",
    "section": "Gibb sampling algorithm (Ex)",
    "text": "Gibb sampling algorithm (Ex)\nThe conditional distributions are\n\\begin{aligned}\n\\alpha\\mid\\beta,s,t &\\sim \\text{Bin}(n, \\beta) \\\\\n\\beta\\mid\\alpha,s,t &\\sim \\text{Beta}(\\alpha+s, n-\\alpha+t)\n\\end{aligned}"
  },
  {
    "objectID": "lectures/abm3.html#gibb-sampling-algorithm-ex-2",
    "href": "lectures/abm3.html#gibb-sampling-algorithm-ex-2",
    "title": "Gibbs samplers and variable selection",
    "section": "Gibb sampling algorithm (Ex)",
    "text": "Gibb sampling algorithm (Ex)\n\n\nCode\nlibrary(dygraphs)\n\nsample_alpha_conditional &lt;- function(beta, n) {\n  \n  rbinom(n = 1, size = n, prob = beta)\n  \n}\n\nsample_beta_conditional &lt;- function(alpha, s, t, n) {\n  \n  rbeta(n = 1, shape1 = alpha+s, shape2 = n-alpha+t)\n\n  }\n\ngibbs &lt;- function(N, beta0, data, burnin = 1){\n  \n  alphas &lt;- rep(0, N+1)\n  betas &lt;- rep(beta0, N+1)\n  \n  for (t in 2:(N+1)){\n    alphas[t] &lt;- sample_alpha_conditional(betas[t-1], data$n)\n    betas[t] &lt;- sample_beta_conditional(alphas[t], data$s, data$t, data$n)\n  }\n  \n  return(tibble(alpha = alphas[(burnin+1):N], \n                beta = betas[(burnin+1):N], \n                iter = (burnin+1):N)\n         )\n}\n\nN &lt;- 5000\ndata &lt;- list(n = 5, s = 2, t = 1)\n\nsamples &lt;- gibbs(N, 0.5, data, burnin = 100)\n\n# ggplot(samples) +\n#   geom_line(aes(x=iter, y = beta)) +\n#   theme_bw()\n\ndygraph(ts(samples$alpha,start = 1), ylab = \"Alpha\") %&gt;% dyRangeSelector()"
  },
  {
    "objectID": "lectures/abm3.html#zellners-g-prior-probability-model",
    "href": "lectures/abm3.html#zellners-g-prior-probability-model",
    "title": "Gibbs samplers and variable selection",
    "section": "Zellner‚Äôs G-prior: Probability model",
    "text": "Zellner‚Äôs G-prior: Probability model\nPrior\n\\begin{aligned}\n\\beta \\mid \\sigma^2, X &\\sim \\mathcal N\\left(0, g\\sigma^2(X^\\top X)^{-1}\\right)\\\\\n\\pi(\\sigma^2)&\\propto \\sigma^{-2} \\end{aligned}\nData model\ny \\mid \\beta, \\sigma^2,X \\sim \\mathcal{N}(X \\beta, \\sigma^2) with fixed hyper-parameter g (Zellner 1986)"
  },
  {
    "objectID": "lectures/abm3.html#zellners-g-prior-posterior",
    "href": "lectures/abm3.html#zellners-g-prior-posterior",
    "title": "Gibbs samplers and variable selection",
    "section": "Zellner‚Äôs G-prior: Posterior",
    "text": "Zellner‚Äôs G-prior: Posterior\nZellner‚Äôs G-prior leads to a posterior\n\\begin{aligned}\n\\beta \\mid \\sigma^2,y,X&\\sim \\mathcal{N}\\left(\\frac{g}{g+1}\\hat\\beta,\n\\frac{\\sigma^2g}{g+1}(X^\\top X)^{-1}\\right) \\\\\n\\sigma^2\\mid y,X &\\sim \\mathcal{IG}\\left(\\frac{n}{2},\\frac{s^2}{2}+\\frac{1}{2(g+1)}\n\\hat\\beta^\\top X^\\top X\\hat\\beta\\right)   \\end{aligned}"
  },
  {
    "objectID": "lectures/abm3.html#zellners-g-prior-model-evidence",
    "href": "lectures/abm3.html#zellners-g-prior-model-evidence",
    "title": "Gibbs samplers and variable selection",
    "section": "Zellner‚Äôs G-prior: Model evidence",
    "text": "Zellner‚Äôs G-prior: Model evidence\n\\begin{aligned} m(y\\mid X) = & (g+1)^{-(p+1)/2}\\pi^{-n/2}\\Gamma(n/2) \\\\ & \\times\\left[y^\\top y-\\frac{g}{g+1}y^\\top X(X^\\top X)^{-1}X^\\top y\\right]^{-n/2}\\end{aligned}"
  },
  {
    "objectID": "lectures/abm3.html#zellners-g-prior-exact-samples",
    "href": "lectures/abm3.html#zellners-g-prior-exact-samples",
    "title": "Gibbs samplers and variable selection",
    "section": "Zellner‚Äôs G-prior: Exact samples",
    "text": "Zellner‚Äôs G-prior: Exact samples\nFor1 i\\in\\{1,2,\\ldots,N\\}\n\nSample \\sigma^2_i \\mid y,X from IG\nSample \\beta_i \\mid \\sigma^2_i,y,X from MVN\n\nResults in exact posterior draws \\theta_i = (\\beta_i,\\sigma^2_i)\nSample in parallel if desired"
  },
  {
    "objectID": "lectures/abm3.html#model-averaging-with-zellners-g-prior",
    "href": "lectures/abm3.html#model-averaging-with-zellners-g-prior",
    "title": "Gibbs samplers and variable selection",
    "section": "Model averaging with Zellner‚Äôs G-prior",
    "text": "Model averaging with Zellner‚Äôs G-prior\nModel choice involves selecting best set of covariates\nBayesian model averaging finds best subset of models with uncertainty\n\n\\gamma_i \\in \\{0,1\\} for i \\in \\{1,\\ldots,p\\}\n\\gamma=(\\gamma_1, \\gamma_2, \\ldots, \\gamma_p)\nX_\\gamma be the submatrix of X which only includes covariate x_i as a column when \\gamma_i=1\nAlways keep the intercept"
  },
  {
    "objectID": "lectures/abm3.html#probability-model-with-averaging-v1",
    "href": "lectures/abm3.html#probability-model-with-averaging-v1",
    "title": "Gibbs samplers and variable selection",
    "section": "Probability model with averaging (V1)",
    "text": "Probability model with averaging (V1)\nPrior\n\\begin{aligned}\n\\beta_\\gamma \\mid \\sigma^2, \\gamma, X &\\sim \\mathcal N\\left(0, g\\sigma^2(X_\\gamma^\\top X_\\gamma)^{-1}\\right),\\\\\n\\pi(\\sigma^2\\mid \\gamma)&\\propto \\sigma^{-2} \\\\\n\\gamma_i & \\overset{\\text{iid}}{\\sim} \\text{Bern}(\\rho), \\quad i\\in\\{1,2,\\ldots,p\\}\n\\end{aligned}\nData model\ny \\mid \\beta,\\sigma^2,\\gamma,X \\sim \\mathcal{N}(X_\\gamma \\beta_\\gamma, \\sigma^2)"
  },
  {
    "objectID": "lectures/abm3.html#probability-model-with-averaging-v2",
    "href": "lectures/abm3.html#probability-model-with-averaging-v2",
    "title": "Gibbs samplers and variable selection",
    "section": "Probability model with averaging (V2)",
    "text": "Probability model with averaging (V2)\nPrior\n\\begin{aligned}\n\\beta_\\gamma \\mid \\sigma^2, \\gamma, X &\\sim \\mathcal N\\left(0, g\\sigma^2(X_\\gamma^\\top X_\\gamma)^{-1}\\right),\\quad \\beta_{1-\\gamma} \\vert \\gamma = 0 \\\\\n\\pi(\\sigma^2\\mid \\gamma)&\\propto \\sigma^{-2} \\\\\n\\gamma_i & \\overset{\\text{iid}}{\\sim} \\text{Bern}(\\rho), \\quad i\\in\\{1,2,\\ldots,p\\}\n\\end{aligned}\nData model\ny \\mid \\beta,\\sigma^2,X \\sim \\mathcal{N}(X \\beta, \\sigma^2)"
  },
  {
    "objectID": "lectures/abm3.html#conditional-posterior-density",
    "href": "lectures/abm3.html#conditional-posterior-density",
    "title": "Gibbs samplers and variable selection",
    "section": "Conditional posterior density",
    "text": "Conditional posterior density\nWe index each model using \\gamma vector. The model \\mathcal{M}_\\gamma has posterior density\n\\pi(\\beta,\\sigma^2 \\mid \\gamma,y,X)\nwhen \\gamma_i =0 then \\beta_i=0, only coordinates \\beta_\\gamma are ‚Äúactive‚Äù"
  },
  {
    "objectID": "lectures/abm3.html#conditional-posterior-distribution",
    "href": "lectures/abm3.html#conditional-posterior-distribution",
    "title": "Gibbs samplers and variable selection",
    "section": "Conditional posterior distribution",
    "text": "Conditional posterior distribution\nThe model \\mathcal{M}_\\gamma has conditional distribution\n\\begin{aligned}\n\\beta_{\\gamma} \\mid \\sigma^2,\\gamma,y,X&\\sim \\mathcal{N}\\left(\\frac{g}{g+1}\\hat\\beta_\\gamma,\n\\frac{\\sigma^2g}{g+1}(X_\\gamma^\\top X_\\gamma)^{-1}\\right) \\\\\n\\sigma^2\\mid \\gamma, y,X &\\sim \\mathcal{IG}\\left(\\frac{n}{2},\\frac{s^2_\\gamma}{2}+\\frac{1}{2(g+1)}\n\\hat\\beta_\\gamma^\\top X_\\gamma^\\top X_\\gamma\\hat\\beta_\\gamma\\right)   \\end{aligned}\nwith \\beta_{1-\\gamma} =0"
  },
  {
    "objectID": "lectures/abm3.html#model-evidence",
    "href": "lectures/abm3.html#model-evidence",
    "title": "Gibbs samplers and variable selection",
    "section": "Model evidence",
    "text": "Model evidence\nThe model evidence for \\mathcal{M}_\\gamma is\n\\begin{aligned} m(y\\mid \\gamma,X) = & (g+1)^{-(p_\\gamma+1)/2}\\pi^{-n/2}\\Gamma(n/2) \\\\ & \\times\\left[y^\\top y-\\frac{g}{g+1}y^\\top X_\\gamma(X_\\gamma^\\top X_\\gamma)^{-1}X_\\gamma^\\top y\\right]^{-n/2}\\end{aligned}\nwhere p_\\gamma = \\sum_{i=1}^p\\gamma_i."
  },
  {
    "objectID": "lectures/abm3.html#full-posterior-over-models",
    "href": "lectures/abm3.html#full-posterior-over-models",
    "title": "Gibbs samplers and variable selection",
    "section": "Full posterior over models",
    "text": "Full posterior over models\nThe posterior density factorises as\n\\pi(\\beta, \\sigma^2, \\gamma \\mid X,y) = \\pi(\\beta, \\sigma^2 \\mid \\gamma, X,y) \\pi(\\gamma \\mid X,y) \nFull posterior sampling strategy\n\nSample over models \\mathcal{M}_\\gamma using \\gamma \\mid X,y\nUse exact sampling for \\beta, \\sigma^2 \\mid \\gamma, X,y"
  },
  {
    "objectID": "lectures/abm3.html#marginal-posterior-for-models",
    "href": "lectures/abm3.html#marginal-posterior-for-models",
    "title": "Gibbs samplers and variable selection",
    "section": "Marginal posterior for models",
    "text": "Marginal posterior for models\n\\begin{aligned}\\pi(\\gamma \\mid X,y) &= \\frac{\\pi(\\beta, \\sigma^2, \\gamma \\mid X,y)}{\\pi(\\beta, \\sigma^2 \\mid \\gamma, X,y)}\\\\\n&~~\\vdots \\\\\n& \\propto m(y\\mid \\gamma,X) \\pi(\\gamma) \\end{aligned}\nwhere \\pi(\\gamma) is prior PMF for \\gamma."
  },
  {
    "objectID": "lectures/abm3.html#references",
    "href": "lectures/abm3.html#references",
    "title": "Gibbs samplers and variable selection",
    "section": "References",
    "text": "References\n\n\nCasella, George, and Edward I George. 1992. ‚ÄúExplaining the Gibbs Sampler.‚Äù The American Statistician 46 (3): 167‚Äì74.\n\n\nZellner, Arnold. 1986. ‚ÄúOn Assessing Prior Distributions and Bayesian Regression Analysis with g Prior Distributions.‚Äù In Bayesian Inference and Decision Techniques: Essays in Honor of Bruno de Finetti, edited by Prem K. Goel and Arnold Zellner, 6:233‚Äì43. Studies in Bayesian Econometrics and Statistics. New York: Elsevier."
  },
  {
    "objectID": "worksheets/abm-worksheet1.html",
    "href": "worksheets/abm-worksheet1.html",
    "title": "Worksheet 1",
    "section": "",
    "text": "Aims\n\n\n\nPosterior computation with conjugate priors and Monte Carlo methods.\nModel choice via Bayes factors and model averaging."
  },
  {
    "objectID": "worksheets/abm-worksheet1.html#activity-1-getting-started",
    "href": "worksheets/abm-worksheet1.html#activity-1-getting-started",
    "title": "Worksheet 1",
    "section": "Activity 1: Getting started",
    "text": "Activity 1: Getting started\nDownload and import the d√©put√©s data into R.\n\n\nStarting code\nlibrary(readr)\nlibrary(ggplot2)\n\ndeputes &lt;- read_csv(file = \"/path/to/deputes2019.csv\")\n\n\n\nExplore briefly the data (number of individuals, size of groups 1 and 2, histograms‚Ä¶).\nChoose a parametric family, P_\\lambda=P(\\cdot\\mid\\lambda), which seems suitable for these data."
  },
  {
    "objectID": "worksheets/abm-worksheet1.html#activity-2-choosing-priors",
    "href": "worksheets/abm-worksheet1.html#activity-2-choosing-priors",
    "title": "Worksheet 1",
    "section": "Activity 2: Choosing priors",
    "text": "Activity 2: Choosing priors\nWe shall study the two following models, where \\pi is a prior distribution:\n\\begin{array}{c|c}\n\\mathcal{M}_1 & \\mathcal{M}_2\\\\\n\\begin{array}{clc}\nY_i & \\overset{\\text{iid}}{\\sim} & P_\\lambda  \\\\\n\\lambda & \\sim& \\pi\n\\end{array}\n&\n\\begin{array}{clc}\nY_i | Z_i = j & \\overset{\\text{iid}}{\\sim} & P_{\\lambda_j} \\\\\n\\lambda_1 & \\sim& \\pi\\\\\n\\lambda_2 & \\sim& \\pi\\\\\n\\end{array}\n\\end{array}\n\n\nFind a conjugate prior for the chosen family of distributions. Is this family of priors flexible enough? If not, which prior would you choose?\nFind Jeffrey‚Äôs prior for this model. What is the associated posterior?\nDecide what your prior distribution will be.\nPlot the prior of the parameters, and the posterior for the parameters of each model. Repeat with different values of the prior hyperparameters.\nGive a 95% credibility interval for the parameters in each model.\nIn model 2, let r_\\lambda=\\frac{\\lambda_1}{\\lambda_2}. Give a Monte Carlo estimate of the prior and posterior expectation and variance of r_\\lambda."
  },
  {
    "objectID": "worksheets/abm-worksheet1.html#activity-3-bayes-factors",
    "href": "worksheets/abm-worksheet1.html#activity-3-bayes-factors",
    "title": "Worksheet 1",
    "section": "Activity 3: Bayes factors",
    "text": "Activity 3: Bayes factors\nWe would now like to compute the Bayes factor\n\\text{BF}_{21}= \\frac{m_2(y)}{m_1(y)} \\quad \\text{ where }  \\quad\nm_k(y)= \\int _{\\Theta_k} L_k(\\theta_k\\mid y) \\pi_k(\\theta_k) \\mathrm{d} \\theta_k\nWe propose several Monte Carlo methods to calculate the Bayes factor; we would like to compare the methods. For each method, write a script to visualize the convergence of the method.\n\nVanilla Monte Carlo: Give an approximation of B_{21} based on an N-sample of parameters simulated from the prior distribution.\nImportance sampling: Compute the posterior mean and variance of the parameters for each model. Deduce a reasonable importance distribution g to perform importance sampling. Give an approximation of B_{21} in this case.\n\nTry again, using a different importance distribution. What do you observe?\n\nExplicit computation: Give the explicit expression of B_{21}, and write an R script to evaluate it."
  },
  {
    "objectID": "worksheets/abm-worksheet1.html#activity-4-model-choice",
    "href": "worksheets/abm-worksheet1.html#activity-4-model-choice",
    "title": "Worksheet 1",
    "section": "Activity 4: Model choice",
    "text": "Activity 4: Model choice\nCompare the 3 methods from Activity 3 and select the best one.\n\nNow that you have chosen your method, compute the Bayes factor and conclude on which model is the best.\nSuppose that our prior probability for each model is 0.5. What is the posterior probability of each model?\nWe wish to predict the number of questions that will be asked by a new female individual. Draw a posterior sample from the corresponding \\lambda parameter in each of the following cases:\n\nWe choose model \\mathcal M_1.\nWe choose model \\mathcal M_2.\nWe take a mixture of the two models, with weights equal to the posterior probabilities.\n\nGive the posterior mean and variance. Comment."
  },
  {
    "objectID": "worksheets/abm-worksheet1.html#activity-5-more-models",
    "href": "worksheets/abm-worksheet1.html#activity-5-more-models",
    "title": "Worksheet 1",
    "section": "Activity 5: More models",
    "text": "Activity 5: More models\nPerform model choice for other columns: for Y_i, you might look at any of the quantitative variables. For Z_i, you could also use groupe_sigle, which gives political affiliation, or nb_mandats, which gives the number of other elective offices held. In those cases, Z_i can take more than 2 values."
  },
  {
    "objectID": "worksheets/abm-worksheet2.html",
    "href": "worksheets/abm-worksheet2.html",
    "title": "Worksheet 2",
    "section": "",
    "text": "Aims\n\n\n\nBayesian logistic regression with Metropolis-Hastings algorithm.\nBuilding a model from first principles."
  },
  {
    "objectID": "worksheets/abm-worksheet2.html#activity-1-getting-started",
    "href": "worksheets/abm-worksheet2.html#activity-1-getting-started",
    "title": "Worksheet 2",
    "section": "Activity 1: Getting started",
    "text": "Activity 1: Getting started\nExplore the data and represent graphically the success frequency as a function of distance.\n\n\nStarting code\nlibrary(readr)\nlibrary(ggplot2)\n\ngolf2 &lt;- read_csv(file = \"/path/to/golf2.txt\")"
  },
  {
    "objectID": "worksheets/abm-worksheet2.html#activity-2-bayesian-logistic-regression",
    "href": "worksheets/abm-worksheet2.html#activity-2-bayesian-logistic-regression",
    "title": "Worksheet 2",
    "section": "Activity 2: Bayesian logistic regression",
    "text": "Activity 2: Bayesian logistic regression\nWe consider a logistic regression model with probability \\theta_i\\in (0,1):\n\\begin{aligned}\nY_i\\mid\\theta_i &\\sim \\text{Bern}(\\theta_i)\\\\\n\\text{logit}(\\theta_i) &= \\beta_0+\\beta_1 x_i\n\\end{aligned}\nor alternatively represented by P[Y_i=1] = \\text{logit}^{-1}(\\beta_0+\\beta_1 x_i) = \\frac{e^{\\beta_0+\\beta_1 x_i}}{1+e^{\\beta_0+\\beta_1 x_i}}.\n\nWhat are reasonable priors for \\beta_0 and \\beta_1?\nWrite the likelihood and posterior associated with this model. Is the posterior distribution easy to sample from?\nSimulate a pseudo-sample from this posterior via MCMC thanks to the MCMClogit function in package {MCMCpack}. Check convergence and adapt the algorithmic parameters as necessary.\nRepeat with another prior and compare the results."
  },
  {
    "objectID": "worksheets/abm-worksheet2.html#activity-3-modelling-building",
    "href": "worksheets/abm-worksheet2.html#activity-3-modelling-building",
    "title": "Worksheet 2",
    "section": "Activity 3: Modelling building",
    "text": "Activity 3: Modelling building\nNow we will build a model for the data from first principles.\n\nThe ball radius is r=0.07 and the hole radius is R=0.177 (all distance are in feet). Propose a model based on the geometric properties of the problem, assuming the player hits the ball with some random angle \\alpha.\nWrite the likelihood of your model, and choose a prior for any parameters.\nGet a pseudo-sample of the posterior thanks to the function MCMCpack::MCMCmetrop1R. Assess convergence.\nModify the algorithmic parameters (tune, burnin, ‚Ä¶) until the output is acceptable. What is the impact of the starting point theta.init?\nCompare the data fit of both models.\nPropose an extension of the second model, and check whether the fit is improved."
  },
  {
    "objectID": "worksheets/abm-worksheet3.html",
    "href": "worksheets/abm-worksheet3.html",
    "title": "Worksheet 3",
    "section": "",
    "text": "Aims\n\n\n\nSelection of explanatory variables in a linear regression setting, through exact computation and Gibbs‚Äô sampling."
  },
  {
    "objectID": "worksheets/abm-worksheet3.html#activity-1-getting-started",
    "href": "worksheets/abm-worksheet3.html#activity-1-getting-started",
    "title": "Worksheet 3",
    "section": "Activity 1: Getting started",
    "text": "Activity 1: Getting started\nDownload and import the death rate data into R. Explore the data visually and determine the correlations between covariates and with the dependent variable deathrate.\n\n\nStarting code\nlibrary(readr)\nlibrary(ggplot2)\n\ndeathrate &lt;- read_csv(file = \"/path/to/deathrate2.csv\", col_types = cols(col_skip(), col_guess()))\n# Discards the first row."
  },
  {
    "objectID": "worksheets/abm-worksheet3.html#activity-2-frequentist-inference",
    "href": "worksheets/abm-worksheet3.html#activity-2-frequentist-inference",
    "title": "Worksheet 3",
    "section": "Activity 2: Frequentist inference",
    "text": "Activity 2: Frequentist inference\nOur linear model is (with p=20, n=60) \\begin{aligned}\ny_i&=\\beta_0 + \\beta_1x_i^1 + \\beta_2 x_i^2 + \\ldots + \\beta_{p}x_i^{p} + \\epsilon_i\\\\\n\\epsilon_i & \\sim  \\mathcal N(0,\\sigma^2)\\end{aligned}\nCalculate the Frequentist estimates of \\beta and \\sigma^2, which we denote \\hat\\beta and s^2. Comment on the statistically significant variables.\n\n\nStarting code\ndr_lm &lt;- lm(deathrate ~ ., data = deathrate)\nsummary(dr_lm)\n\n# calculate model matrix for Activity 3\nX &lt;- model.matrix(deathrate ~ ., data = deathrate)"
  },
  {
    "objectID": "worksheets/abm-worksheet3.html#activity-3-bayesian-inference-using-zellners-g-prior",
    "href": "worksheets/abm-worksheet3.html#activity-3-bayesian-inference-using-zellners-g-prior",
    "title": "Worksheet 3",
    "section": "Activity 3: Bayesian inference using Zellner‚Äôs G-prior",
    "text": "Activity 3: Bayesian inference using Zellner‚Äôs G-prior\nConsider the following prior simplified from Zellner (1986) for \\beta \\in \\mathbb{R}^{p+1}, \\begin{aligned}\n\\beta \\mid \\sigma^2, X &\\sim \\mathcal N\\left(0, g\\sigma^2(X^\\top X)^{-1}\\right)\\\\\n\\pi(\\sigma^2)&\\propto \\sigma^{-2} \\end{aligned}\nThis prior is conjugate and the associated posterior is\n\\begin{aligned}\n\\beta \\mid \\sigma^2,y,X&\\sim \\mathcal{N}\\left(\\frac{g}{g+1}\\hat\\beta,\n\\frac{\\sigma^2g}{g+1}(X^\\top X)^{-1}\\right)\\\\\n\\sigma^2\\mid y,X &\\sim \\mathcal{IG}\\left(\\frac{n}{2},\\frac{s^2}{2}+\\frac{1}{2(g+1)}\n\\hat\\beta^\\top X^\\top X\\hat\\beta\\right)  \\end{aligned}\nwhere \\mathcal{IG}(a,b) is the inverse Gamma distribution with shape a and scale b. The marginal posterior distribution of \\beta is\n\\begin{aligned}\n\\beta|y,X &\\sim& \\mathcal{T}\\left(n,\\frac{g}{g+1}\\hat\\beta,\\frac{g(s^2+\\hat\\beta^\\top X^\\top\nX\\hat\\beta/(g+1))}{n(g+1)}(X^\\top X)^{-1}\\right)  \\end{aligned}\nwhere \\mathcal{T} denotes the multivariate t-distribution.\nLet \\gamma=(\\gamma_1, \\gamma_2, \\ldots, \\gamma_p) for \\gamma_i \\in \\{0,1\\} and X_\\gamma be the submatrix of X which only includes covariate x_i as a column when \\gamma_i=1 (always keeping the intercept). For a model considering covariate i such that \\gamma_i=1, the marginal likelihood is\nm(y|X, \\gamma) = (g+1)^{-(p_\\gamma+1)/2}\\pi^{-n/2}\\Gamma(n/2)  \\left[y^\\top y-\\frac{g}{g+1}y^\\top X_\\gamma(X_\\gamma^\\top X_\\gamma)^{-1}X_\\gamma^\\top y\\right]^{-n/2} where p_\\gamma = \\sum_{i=1}^p\\gamma_i, the number of non-zero coefficients (excluding the intercept).\n\nFor g=0.1, 1, 10, 100, 1000, give \\mathbb{E}[\\sigma^2|y, X] and \\mathbb{E}[\\beta_0|y, X]. What can you conclude about the impact of the prior on the posterior?\nWe would like to test the hypothesis H_0: \\beta_7=\\beta_8=0. Compute Bayes‚Äô factor given our data and conclude, using Jeffreys‚Äô scale of evidence."
  },
  {
    "objectID": "worksheets/abm-worksheet3.html#activity-4-model-choice-with-exact-computation",
    "href": "worksheets/abm-worksheet3.html#activity-4-model-choice-with-exact-computation",
    "title": "Worksheet 3",
    "section": "Activity 4: Model choice with exact computation",
    "text": "Activity 4: Model choice with exact computation\nIn this activity, we restrict ourselves to the first 3 explanatory variables.\n\n\nStarting code\nX1 &lt;- X[ , 1:4]\n\n\nWe would like to know which variables to include in our model, and assume that the intercept is necessarily included. We have 2^3=8 possible models. To each model we associate the variable \\gamma=[\\gamma_1, \\gamma_2, \\gamma_3] where \\gamma_i=1 if x^i is included in the model, and \\gamma_i=0 otherwise.\nUsing the marginal probability formula of Activity 3, compute the marginal likelihood m(y|X_\\gamma). Deduce the most likely model a posteriori."
  },
  {
    "objectID": "worksheets/abm-worksheet3.html#activity-5-model-choice-using-gibbs-sampling",
    "href": "worksheets/abm-worksheet3.html#activity-5-model-choice-using-gibbs-sampling",
    "title": "Worksheet 3",
    "section": "Activity 5: Model choice using Gibbs‚Äô sampling",
    "text": "Activity 5: Model choice using Gibbs‚Äô sampling\nWe now consider all p explanatory variables. We thus need to choose between 2^p models.\n\nUse the bms function from package {BMS} to sample from the posterior distribution of \\gamma, and conclude on the most likely model.\nFor a new value of your choice of the covariates, e.g.¬†x_{n+1} = [x_{n+1}^1,x_{n+1}^2, \\ldots,x_{n+1}^p], perform prediction using\n\nthe most probable model a posteriori\nthe best model as selected by AIC\na mixture of models weighted by their posterior probabilities."
  },
  {
    "objectID": "worksheets/abm-worksheet3.html#footnotes",
    "href": "worksheets/abm-worksheet3.html#footnotes",
    "title": "Worksheet 3",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nData from McDonald and Schwing (1973), Gunst and Mason (1980), Sp√§th (1992)‚Ü©Ô∏é"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Bayesian Modelling",
    "section": "",
    "text": "Course website for M2 MASH course on Applied Bayesian Modelling."
  },
  {
    "objectID": "index.html#slides",
    "href": "index.html#slides",
    "title": "Applied Bayesian Modelling",
    "section": "Slides",
    "text": "Slides\nLecture slides with main content. Further demonstrations in class.\n\nSection 1: Bayes & Monte Carlo [Reading: M&R Ch. 2]\nSection 2: MCMC & Metropolis-Hastings [Reading: M&R Ch. 4]\nSection 3: Gibbs & Variable Selection [Reading: M&R Ch. 3]\nSection 4\nSection 5"
  },
  {
    "objectID": "index.html#worksheets",
    "href": "index.html#worksheets",
    "title": "Applied Bayesian Modelling",
    "section": "Worksheets",
    "text": "Worksheets\nWorksheets with practical and computer activities based in R.\n\nWorksheet 1\nWorksheet 2\nWorksheet 3\nWorksheet 4\nWorksheet 5"
  },
  {
    "objectID": "index.html#code",
    "href": "index.html#code",
    "title": "Applied Bayesian Modelling",
    "section": "Code",
    "text": "Code\nR files from live-coding in class.\n\nWorksheet 1\nWorksheet 2\nWorksheet 3"
  },
  {
    "objectID": "index.html#datasets",
    "href": "index.html#datasets",
    "title": "Applied Bayesian Modelling",
    "section": "Datasets",
    "text": "Datasets\nDatasets for the worksheets. See worksheets for explanation.\n Download data D√©put√©s 2019\n Download data Golf\n Download data Death rates"
  },
  {
    "objectID": "index.html#additional-resources",
    "href": "index.html#additional-resources",
    "title": "Applied Bayesian Modelling",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nTextbooks\n\nBayesian Data Analysis (Gelman et al. 2013)\n\n\n\nPapers\n\n‚ÄúVisualization in Bayesian Workflow‚Äù (Gabry et al. 2019)\n‚ÄúBayesian Workflow‚Äù (Gelman et al. 2020)\n\n\n\nTutorials\n\n‚ÄúLinear regression‚Äù Stan User‚Äôs Guide\n‚ÄúLogistic and probit regression‚Äù Stan User‚Äôs Guide"
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Applied Bayesian Modelling",
    "section": "References",
    "text": "References\n\n\nGabry, Jonah, Daniel Simpson, Aki Vehtari, Michael Betancourt, and Andrew Gelman. 2019. ‚ÄúVisualization in Bayesian Workflow.‚Äù Journal of the Royal Statistical Society Series A: Statistics in Society 182 (2): 389‚Äì402. https://doi.org/10.1111/rssa.12378.\n\n\nGelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2013. Bayesian Data Analysis. Chapman & Hall/CRC Press. http://www.stat.columbia.edu/~gelman/book/BDA3.pdf.\n\n\nGelman, Andrew, Aki Vehtari, Daniel Simpson, Charles C Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian B√ºrkner, and Martin Modr√°k. 2020. ‚ÄúBayesian Workflow.‚Äù arXiv Preprint arXiv:2011.01808. https://doi.org/10.48550/arXiv.2011.01808.\n\n\nMarin, Jean-Michel, and Christian P Robert. 2014. Bayesian Essentials with R. Vol. 48. Springer. https://bu.dauphine.psl.eu/bibliodata.html?record_id=TN_cdi_hal_primary_oai_HAL_hal_01337395v1&rtype=book&online=true&action=view_record."
  },
  {
    "objectID": "lectures/abm2.html#applied-bayesian-modelling",
    "href": "lectures/abm2.html#applied-bayesian-modelling",
    "title": "MCMC & Metropolis-Hastings",
    "section": "Applied Bayesian modelling",
    "text": "Applied Bayesian modelling\n\nStatistical inference üîç\nStatistical prediction\nBayesian models\nComputation üîç\nWorkflows"
  },
  {
    "objectID": "lectures/abm2.html#example-2-golf-data",
    "href": "lectures/abm2.html#example-2-golf-data",
    "title": "MCMC & Metropolis-Hastings",
    "section": "Example 2: Golf data",
    "text": "Example 2: Golf data\n\n\nCode\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ngolf &lt;- read_csv(file = josh_data_dir(\"golf2.txt\"))\n# Download: https://github.com/bonStats/mash-abm/tree/main/data\n# change location to match folder of 'golf2.txt' on your computer\n# e.g. \"~/Downloads/golf2.txt\" or \"C:/Users/[User Name]/Downloads/golf2.txt\"\n\ngolf %&gt;% group_by(distance) %&gt;% summarise(prop_success = mean(success)) %&gt;%\nggplot() + \n  geom_col(aes(x = distance, y = prop_success)) +\n  scale_x_continuous(\"Distance to hole\") +\n  scale_y_continuous(\"Proportion successful\") +\n  theme_bw()"
  },
  {
    "objectID": "lectures/abm2.html#statistical-inference-1",
    "href": "lectures/abm2.html#statistical-inference-1",
    "title": "MCMC & Metropolis-Hastings",
    "section": "Statistical inference",
    "text": "Statistical inference\n\n\n\n\n\n\nLay question\n\n\nWhat determines accuracy in golf putting?\n\n\n\n\n\n\n\n\n\nStatistical questions\n\n\n\nHow likely is the player to successfully sink the putt at distance x metres?\nHow variable is the players angle of putting compared to the correct angle?\n\n\n\n\n\nAccording to this putting data from one player"
  },
  {
    "objectID": "lectures/abm2.html#logistic-regression",
    "href": "lectures/abm2.html#logistic-regression",
    "title": "MCMC & Metropolis-Hastings",
    "section": "Logistic regression",
    "text": "Logistic regression\nModel for Y_i \\in \\{0,1\\}, for i \\in \\{1,\\ldots,n\\}:\n\\Pr(Y_i = 1) = \\frac{\\exp\\{\\beta_0 + \\beta_1x_i\\}}{1 + \\exp\\{\\beta_0 + \\beta_1x_i\\}}\n\nCovariate x_i (can be many)\nUnknown coefficients \\beta_0,\\beta_1 (posterior distribution)"
  },
  {
    "objectID": "lectures/abm2.html#logistic-regression-1",
    "href": "lectures/abm2.html#logistic-regression-1",
    "title": "MCMC & Metropolis-Hastings",
    "section": "Logistic regression",
    "text": "Logistic regression\n\n\nCode\nn &lt;- 20\n\nb0 &lt;- 0.5\nb1 &lt;- 2\nlink &lt;- function(x){\n  exp(b0 + b1 * x) / ( 1 + exp(b0 + b1 * x) )\n}\n\nsimdata &lt;- tibble(x = rnorm(n))\n\npr_y_1 &lt;- link(simdata$x)\n\nsimdata &lt;- simdata %&gt;% mutate(y = rbinom(n, size = 1, prob = pr_y_1))\n\nggplot(simdata) + \n  geom_point(aes(x=x,y=y, colour = \"Observed data\")) + \n  stat_function(aes(colour = \"Pr(Y=1)\"), fun = link) + \n  scale_color_discrete(\"\") + \n  theme_bw()"
  },
  {
    "objectID": "lectures/abm2.html#recap-monte-carlo-approximation",
    "href": "lectures/abm2.html#recap-monte-carlo-approximation",
    "title": "MCMC & Metropolis-Hastings",
    "section": "Recap: Monte Carlo approximation",
    "text": "Recap: Monte Carlo approximation\n\\mathbb{E}_{\\theta\\sim P}\\left[f(\\theta)\\right] \\approx \\frac{1}{N}\\sum_{i=1}^{N}f(\\theta_i), \\quad \\text{where}~\\theta_i \\overset{\\text{iid}}{\\sim} P\nWhat if we can‚Äôt sample from P?"
  },
  {
    "objectID": "lectures/abm2.html#self-normalised-importance-sampling",
    "href": "lectures/abm2.html#self-normalised-importance-sampling",
    "title": "MCMC & Metropolis-Hastings",
    "section": "Self-normalised importance sampling",
    "text": "Self-normalised importance sampling\nExtending IS when \\pi(\\cdot\\mid y) can‚Äôt be evaluated due to Z.\n\\mathbb{E}_{\\theta\\sim \\Pi(\\cdot\\mid y)}\\left[f(\\theta)\\right] = \\frac{\\mathbb{E}_{\\theta\\sim Q}\\left[f(\\theta)w(\\theta)\\right]}{\\mathbb{E}_{\\theta\\sim Q}\\left[w(\\theta)\\right]}\nwhere w(\\theta) = \\frac{L(\\theta\\mid y)\\pi(\\theta)}{q(\\theta)}."
  },
  {
    "objectID": "lectures/abm2.html#self-normalised-importance-sampling-1",
    "href": "lectures/abm2.html#self-normalised-importance-sampling-1",
    "title": "MCMC & Metropolis-Hastings",
    "section": "Self-normalised importance sampling",
    "text": "Self-normalised importance sampling\n\\mathbb{E}_{\\theta\\sim \\Pi(\\cdot\\mid y)}\\left[f(\\theta)\\right] \\approx \\frac{1}{N}\\sum_{i=1}^{N}f(\\theta_i)\\bar{w}_i\n\n\\theta_i \\overset{\\text{iid}}{\\sim} Q\n\\bar{w}_i = \\frac{w(\\theta_i)}{\\sum_{j=1}^N w(\\theta_j)}\nw(\\theta) = \\frac{L(\\theta\\mid y)\\pi(\\theta)}{q(\\theta)} \\propto \\frac{\\Pi(\\theta\\mid y)}{q(\\theta)}"
  },
  {
    "objectID": "lectures/abm2.html#markov-chain-monte-carlo",
    "href": "lectures/abm2.html#markov-chain-monte-carlo",
    "title": "MCMC & Metropolis-Hastings",
    "section": "Markov chain Monte Carlo",
    "text": "Markov chain Monte Carlo\nExtending vanilla MC when \\theta \\sim \\Pi(\\cdot\\mid y) can‚Äôt be sampled.\nConstruct Markov chain with limiting distribution \\Pi(\\cdot\\mid y)\n\\theta_t \\sim K(\\cdot\\mid \\theta_{t-1}),\\quad t\\in\\{1,2,\\ldots,\\}\nfor some \\theta_0.\n\\theta_n \\sim K^n(\\cdot\\mid\\theta_0)\nfor \\theta_n \\overset{\\text{d}}{\\longrightarrow} \\Pi(\\cdot\\mid y ) as n\\rightarrow \\infty"
  },
  {
    "objectID": "lectures/abm2.html#markov-chain-monte-carlo-1",
    "href": "lectures/abm2.html#markov-chain-monte-carlo-1",
    "title": "MCMC & Metropolis-Hastings",
    "section": "Markov chain Monte Carlo",
    "text": "Markov chain Monte Carlo\nWe can estimate quantities with the MCMC samples\n\\mathbb{E}_{\\theta\\sim \\Pi(\\cdot\\mid y)}\\left[f(\\theta)\\right] \\approx \\sum_{t=1}^{N}f(\\theta_t),\\quad \\theta_t \\sim K(\\cdot\\mid \\theta_{t-1})\nWhat to use for K?"
  },
  {
    "objectID": "lectures/abm2.html#the-metropolis-hastings-algorithm",
    "href": "lectures/abm2.html#the-metropolis-hastings-algorithm",
    "title": "MCMC & Metropolis-Hastings",
    "section": "The Metropolis-Hastings algorithm",
    "text": "The Metropolis-Hastings algorithm\nInitialise \\theta_0. At time t:\n\nSample proposal \\theta^\\prime \\sim Q(\\cdot\\mid\\theta_{t-1})\nCalculate acceptance \\alpha = \\min \\left\\{\\frac{q(\\theta_{t-1} \\mid \\theta^\\prime)\\pi(\\theta^\\prime\\mid y)}{q(\\theta^\\prime \\mid \\theta_{t-1})\\pi(\\theta_{t-1}\\mid y)},1 \\right\\}\nDraw U \\sim \\text{Unif}(0,1)\n\n\nIf \\alpha \\geq U accept proposal, set \\theta_t = \\theta^\\prime\nElse \\alpha&lt;U reject proposal, set \\theta_t = \\theta_{t-1}\n\nRun until convergence (check diagnostics)"
  },
  {
    "objectID": "lectures/abm2.html#mh-acceptance-rate",
    "href": "lectures/abm2.html#mh-acceptance-rate",
    "title": "MCMC & Metropolis-Hastings",
    "section": "MH: Acceptance rate",
    "text": "MH: Acceptance rate\n\\alpha = \\min \\left\\{\\frac{q(\\theta_{t-1} \\mid \\theta^\\prime)\\pi(\\theta^\\prime\\mid y)}{q(\\theta^\\prime \\mid \\theta_{t-1})\\pi(\\theta_{t-1}\\mid y)},1 \\right\\}  = \\min \\left\\{\\frac{q(\\theta_{t-1} \\mid \\theta^\\prime)L(\\theta^\\prime\\mid y)\\pi(\\theta^\\prime)}{q(\\theta^\\prime \\mid \\theta_{t-1})L(\\theta_{t-1}\\mid y)\\pi(\\theta_{t-1})},1 \\right\\}\n\nNo normalising constant required!\nSpecial case: Symmetric proposal"
  },
  {
    "objectID": "lectures/abm2.html#running-a-mh-algorithm",
    "href": "lectures/abm2.html#running-a-mh-algorithm",
    "title": "MCMC & Metropolis-Hastings",
    "section": "Running a MH Algorithm",
    "text": "Running a MH Algorithm\n\n\nCode\nlibrary(dygraphs)\n\nlogprob_target &lt;- function(theta){ # unnormalised if neccessary\n  \n  dnorm(theta, mean = 1, sd = 1, log = TRUE)\n  \n}\n\nrproposal &lt;- function(given_theta, tune_sd) {\n  \n  rnorm(1, mean = given_theta, sd = tune_sd)\n  \n}\n\nlogprob_proposal &lt;- function(theta, given_theta, tune_sd){\n  \n  dnorm(theta, mean = given_theta, sd = tune_sd, log = TRUE)\n  \n}\n\nmh_iteration &lt;- function(given_theta, tune_sd){\n  \n  proposal &lt;- rproposal(given_theta, tune_sd)\n  log_uniform &lt;- log(runif(1))\n  \n  log_alpha &lt;- ( logprob_proposal(given_theta, proposal, tune_sd) + logprob_target(proposal) ) - \n                ( logprob_proposal(proposal, given_theta, tune_sd) + logprob_target(given_theta) )\n  \n  if (log_alpha &gt;= log_uniform){\n    next_theta &lt;- proposal\n  } else {\n    next_theta &lt;- given_theta\n  }\n  \n  return(next_theta)\n}\n\nmh &lt;- function(n, theta0, tune_sd, burnin = 0){\n  \n  thetas &lt;- rep(theta0, n)\n  \n  for (t in 2:n){\n    thetas[t] &lt;- mh_iteration(thetas[t-1], tune_sd)\n  }\n  \n  return(thetas[(burnin+1):n])\n}\n\nN &lt;- 5000\nsimdata &lt;- tibble(theta = mh(N, 0, 1), iter = 1:N)\n\n# ggplot(simdata) + \n#   geom_line(aes(x=iter, y = theta)) + \n#   theme_bw()\n\ndygraph(ts(simdata$theta,start = 1)) %&gt;% dyRangeSelector()"
  },
  {
    "objectID": "lectures/abm2.html#diagnostics-1",
    "href": "lectures/abm2.html#diagnostics-1",
    "title": "MCMC & Metropolis-Hastings",
    "section": "Diagnostics",
    "text": "Diagnostics\nIn theory convergence is guaranteed as n\\rightarrow \\infty,\n‚ùó however ‚ùó\nin practice we need to monitor the samples.\n\nWe need diagnostics for the samples\nCheck MCMC samples get close enough!"
  },
  {
    "objectID": "lectures/abm2.html#mcmc-chain-trace-plot",
    "href": "lectures/abm2.html#mcmc-chain-trace-plot",
    "title": "MCMC & Metropolis-Hastings",
    "section": "MCMC: Chain trace plot",
    "text": "MCMC: Chain trace plot\n\n\nCode\n# iterate over multiple settings\nlibrary(tidyr)\nlibrary(purrr)\n\ntune_sds &lt;- c(0.01,0.1,0.5,1.0)\nns &lt;- c(200,1000,10000)\nburnins &lt;- c(0,100)\n\nmh_vec &lt;- Vectorize(mh, SIMPLIFY = F)\n\nmhtest &lt;- \n  expand_grid(n = ns, tune_sd = tune_sds, burnin = burnins) %&gt;%\n  mutate(theta = mh_vec(n = n, theta0 = 0, tune_sd = tune_sd, burnin = burnin)) %&gt;%\n  rowwise() %&gt;% mutate(iter = list((burnin+1):n )) %&gt;%\n  unnest(c(theta, iter))\n\nmhtest %&gt;% filter(burnin == 100) %&gt;% rename(sd = tune_sd) %&gt;%\nggplot() + \n  geom_line(aes(x=iter, y = theta)) + \n  theme_bw() + \n  facet_grid(sd ~ n, scales = \"free_x\", labeller = label_both) +\n  ggtitle(\"Burnin = 100\")"
  },
  {
    "objectID": "lectures/abm2.html#mcmc-running-mean-trace-plot",
    "href": "lectures/abm2.html#mcmc-running-mean-trace-plot",
    "title": "MCMC & Metropolis-Hastings",
    "section": "MCMC: Running mean trace plot",
    "text": "MCMC: Running mean trace plot\n\n\nCode\nmhtest %&gt;% filter(burnin == 0) %&gt;% rename(sd = tune_sd) %&gt;%\n  group_by(n, sd, burnin) %&gt;% mutate(theta = cummean(theta)) %&gt;%\nggplot() + \n  geom_line(aes(x=iter, y = theta)) + \n  geom_hline(yintercept = 1.0, linetype = 3) +\n  theme_bw() + \n  facet_grid(sd ~ n, scales = \"free_x\", labeller = label_both) +\n  ggtitle(\"Burnin = 0\")"
  },
  {
    "objectID": "lectures/abm2.html#mcmc-running-mean-trace-plot-1",
    "href": "lectures/abm2.html#mcmc-running-mean-trace-plot-1",
    "title": "MCMC & Metropolis-Hastings",
    "section": "MCMC: Running mean trace plot",
    "text": "MCMC: Running mean trace plot\n\n\nCode\nmhtest %&gt;% filter(burnin == 100) %&gt;% rename(sd = tune_sd) %&gt;%\n  group_by(n, sd, burnin) %&gt;% mutate(theta = cummean(theta)) %&gt;%\nggplot() + \n  geom_line(aes(x=iter, y = theta)) +\n  geom_hline(yintercept = 1.0, linetype = 3) +\n  theme_bw() + \n  facet_grid(sd ~ n, scales = \"free_x\", labeller = label_both) +\n  ggtitle(\"Burnin = 100\")"
  },
  {
    "objectID": "lectures/abm2.html#mh-optimal-acceptance-rate",
    "href": "lectures/abm2.html#mh-optimal-acceptance-rate",
    "title": "MCMC & Metropolis-Hastings",
    "section": "MH: Optimal acceptance rate",
    "text": "MH: Optimal acceptance rate\n\\alpha = 0.234\n\nJustified as dim. d \\rightarrow \\infty (Roberts and Rosenthal 2001)\nStill often used in practice\nSpecific to Metropolis-Hastings algorithm"
  },
  {
    "objectID": "lectures/abm2.html#mh-optimal-acceptance-rate-1",
    "href": "lectures/abm2.html#mh-optimal-acceptance-rate-1",
    "title": "MCMC & Metropolis-Hastings",
    "section": "MH: Optimal acceptance rate",
    "text": "MH: Optimal acceptance rate\n\n\nCode\nmhtest %&gt;% filter(burnin == 100, n == 10000) %&gt;% rename(sd = tune_sd) %&gt;%\n  mutate(accept = c(NA, abs(diff(theta, )) &gt; 1e-08)) %&gt;%\n  filter(!is.na(accept)) %&gt;% \n  group_by(sd) %&gt;% \n  summarise(`Acceptance rate` = mean(accept)) %&gt;%\nggplot() + \n  geom_point(aes(x=sd, y = `Acceptance rate`)) + \n  geom_hline(yintercept = 0.234, linetype = 3) +\n  theme_bw()"
  },
  {
    "objectID": "lectures/abm2.html#recap-monte-carlo-clt",
    "href": "lectures/abm2.html#recap-monte-carlo-clt",
    "title": "MCMC & Metropolis-Hastings",
    "section": "Recap: Monte Carlo CLT",
    "text": "Recap: Monte Carlo CLT\n\\sqrt{N} \\left[ \\frac{1}{N}\\sum_{i=1}^{N}f(\\theta_i) - \\mathbb{E}_{\\theta\\sim P}\\left[f(\\theta)\\right]\\right] \\overset{\\text{d}}{\\longrightarrow} N(0, \\sigma^2_f) if \\theta_i \\overset{\\text{iid}}{\\sim} P, and \\sigma^2_f = \\text{Var}[f(\\theta_i)] &lt; \\infty.\n\nVanilla Monte Carlo error has rate \\frac{\\sigma_f}{\\sqrt{N}} = \\mathcal{O}(N^{-1/2})"
  },
  {
    "objectID": "lectures/abm2.html#mcmc-clt",
    "href": "lectures/abm2.html#mcmc-clt",
    "title": "MCMC & Metropolis-Hastings",
    "section": "MCMC CLT",
    "text": "MCMC CLT\n\\sqrt{N} \\left[ \\frac{1}{N}\\sum_{t=1}^{N}f(\\theta_t) - \\mathbb{E}_{\\theta\\sim P}\\left[f(\\theta)\\right]\\right] \\overset{\\text{d}}{\\longrightarrow} N\\left(0, \\sigma^2_f\\sum_{t=-\\infty}^\\infty \\rho_t\\right)\n\nif \\theta_1 \\sim P, \\theta_t \\sim K(\\cdot\\mid \\theta_{t-1}) for t\\in \\{2,\\ldots,N\\}\nPK=P\n\\sigma^2_f = \\text{Var}[f(\\theta_i)] &lt; \\infty, \\rho_t = \\frac{\\text{cov}[f(\\theta_1),f(\\theta_{1+t})]}{\\sigma^2_f}\nMCMC error rate \\mathcal{O}(N^{-1/2}) at stationarity"
  },
  {
    "objectID": "lectures/abm2.html#mcmc-clt-intuitions",
    "href": "lectures/abm2.html#mcmc-clt-intuitions",
    "title": "MCMC & Metropolis-Hastings",
    "section": "MCMC CLT intuitions",
    "text": "MCMC CLT intuitions\nGood MCMC schemes:\n\nGet to stationary distribution quickly (converge fast)\nHave small autocorrelation \\rho_t\n\nWhat quantitative measures exist to measure this?"
  },
  {
    "objectID": "lectures/abm2.html#mcmc-convergence-hatr-and-friends-1",
    "href": "lectures/abm2.html#mcmc-convergence-hatr-and-friends-1",
    "title": "MCMC & Metropolis-Hastings",
    "section": "MCMC: Convergence \\hat{R} and friends (1)",
    "text": "MCMC: Convergence \\hat{R} and friends (1)\n\\hat{R}, potential scale reduction statistic (Gelman and Rubin 1992)\n\nRun multiple chains from different (random) starting points\n\\hat{R} measures the average variance of samples within each chain with respect to the variance of the pooled samples\n\\hat{R} \\approx 1 provides evidence of convergence\n\\hat{R} &gt; 1 indicates chains have not converged to a common distribution"
  },
  {
    "objectID": "lectures/abm2.html#mcmc-effective-sample-size-2",
    "href": "lectures/abm2.html#mcmc-effective-sample-size-2",
    "title": "MCMC & Metropolis-Hastings",
    "section": "MCMC: Effective sample size (2)",
    "text": "MCMC: Effective sample size (2)\nVanilla Monte Carlo sample size =N\nMCMC effective sample size N_\\text{eff}=\\frac{N}{\\sum_{t=-\\infty}^\\infty\\rho_t} = \\frac{N}{1+2\\sum_{t=1}^\\infty\\rho_t}\n\nLoss of efficiency from using MCMC instead of vanilla Monte Carlo\nEstimate from MCMC samples (valid at stationarity)\n\nN_\\text{eff} \\approx \\frac{N}{1+2\\sum_{t=1}^\\infty\\hat\\rho_t}"
  },
  {
    "objectID": "lectures/abm2.html#effective-sample-size-2",
    "href": "lectures/abm2.html#effective-sample-size-2",
    "title": "MCMC & Metropolis-Hastings",
    "section": "Effective sample size (2)",
    "text": "Effective sample size (2)\n\n\nCode\nlibrary(batchmeans)\n\nmhtest2 &lt;- mhtest %&gt;% filter(burnin == 100) %&gt;% rename(sd = tune_sd)\n\ntheta_ess &lt;- mhtest2 %&gt;% group_by(sd,n) %&gt;% \n  summarise(ess=round(ess(theta),digits = 1)) %&gt;% \n  mutate(x=(n+700)/8)\n\nggplot() + \n  geom_line(aes(x=iter, y = theta), data = mhtest2) + \n  geom_label(aes(x=x, y=4, label=ess), data = theta_ess) + \n  theme_bw() + \n  facet_grid(sd ~ n, scales = \"free_x\", labeller = label_both) +\n  ggtitle(\"Burnin = 100\")"
  },
  {
    "objectID": "lectures/abm2.html#beyond-basic-mcmc",
    "href": "lectures/abm2.html#beyond-basic-mcmc",
    "title": "MCMC & Metropolis-Hastings",
    "section": "Beyond basic MCMC",
    "text": "Beyond basic MCMC\nMetropolis-Hastings\n\nMetropolis Adjusted Langevin Algorithm (MALA) and stochastic gradient versions (Baker et al. 2019)\nOptimal proposal and adaptive Metropolis-Hastings (Rosenthal et al. 2011)\n\nGeneral MCMC\n\nHamiltonian Monte Carlo (HMC) (Betancourt 2017)\nPiecewise Deterministic Markov Process (PDMP) samplers (See links here)"
  },
  {
    "objectID": "lectures/abm2.html#recap-approximate-computation-of-bayesian-models",
    "href": "lectures/abm2.html#recap-approximate-computation-of-bayesian-models",
    "title": "MCMC & Metropolis-Hastings",
    "section": "Recap: Approximate computation of Bayesian models",
    "text": "Recap: Approximate computation of Bayesian models\n\nSelf-normalised importance sampling\nMarkov chain Monte Carlo\nThe Metropolis-Hasting Algorithm\nConvergence diagnostics"
  },
  {
    "objectID": "lectures/abm2.html#references",
    "href": "lectures/abm2.html#references",
    "title": "MCMC & Metropolis-Hastings",
    "section": "References",
    "text": "References\n\n\nBaker, Jack, Paul Fearnhead, Emily B Fox, and Christopher Nemeth. 2019. ‚ÄúSgmcmc: An r Package for Stochastic Gradient Markov Chain Monte Carlo.‚Äù Journal of Statistical Software 91: 1‚Äì27.\n\n\nBetancourt, Michael. 2017. ‚ÄúA Conceptual Introduction to Hamiltonian Monte Carlo.‚Äù arXiv Preprint arXiv:1701.02434.\n\n\nGelman, Andrew, and Donald B Rubin. 1992. ‚ÄúInference from Iterative Simulation Using Multiple Sequences.‚Äù Statistical Science 7 (4): 457‚Äì72.\n\n\nRoberts, Gareth O, and Jeffrey S Rosenthal. 2001. ‚ÄúOptimal Scaling for Various Metropolis-Hastings Algorithms.‚Äù Statistical Science 16 (4): 351‚Äì67.\n\n\nRosenthal, Jeffrey S et al. 2011. ‚ÄúOptimal Proposal Distributions and Adaptive MCMC.‚Äù Handbook of Markov Chain Monte Carlo 4 (10.1201)."
  }
]